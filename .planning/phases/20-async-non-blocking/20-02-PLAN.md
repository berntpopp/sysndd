# Phase 20: Async/Non-blocking - Plan 02

---
phase: 20-async-non-blocking
plan: 02
type: execute
wave: 2
depends_on: ["20-01"]
files_modified:
  - api/endpoints/jobs_endpoints.R
  - api/endpoints/analysis_endpoints.R
  - api/start_sysndd_api.R
autonomous: true

must_haves:
  truths:
    - "Clustering job submission returns HTTP 202 with job ID immediately"
    - "Phenotype clustering job submission returns HTTP 202 with job ID immediately"
    - "Job status endpoint returns current progress for running jobs"
    - "Job status endpoint returns results inline for completed jobs"
    - "Duplicate job submission returns HTTP 409 with existing job ID"
  artifacts:
    - path: "api/endpoints/jobs_endpoints.R"
      provides: "Job submission and status polling endpoints"
      exports: ["POST /clustering/submit", "POST /phenotype_clustering/submit", "GET /<job_id>/status"]
    - path: "api/endpoints/analysis_endpoints.R"
      provides: "Original sync endpoints preserved, new async endpoints added"
      contains: ["functional_clustering", "phenotype_clustering"]
  key_links:
    - from: "api/endpoints/jobs_endpoints.R"
      to: "api/functions/job-manager.R"
      via: "create_job(), get_job_status(), check_duplicate_job()"
      pattern: "create_job\\(|get_job_status\\(|check_duplicate_job\\("
    - from: "api/start_sysndd_api.R"
      to: "api/endpoints/jobs_endpoints.R"
      via: "pr_mount('/api/jobs')"
      pattern: 'pr_mount\\("/api/jobs"'
---

<objective>
Create async job endpoints for clustering analysis and implement job status polling.

Purpose: Convert the long-running clustering operations (STRING-db functional clustering, MCA phenotype clustering) to async pattern that returns HTTP 202 immediately and allows clients to poll for status/results. This prevents API blocking during expensive computations.

Output: New jobs_endpoints.R with submission and polling endpoints, modified analysis_endpoints.R preserving sync versions for backward compatibility, updated start_sysndd_api.R mounting /api/jobs.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/20-async-non-blocking/20-CONTEXT.md
@.planning/phases/20-async-non-blocking/20-RESEARCH.md
@.planning/phases/20-async-non-blocking/20-01-SUMMARY.md

# Files to reference
@api/endpoints/analysis_endpoints.R
@api/functions/job-manager.R
@api/start_sysndd_api.R
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create jobs endpoints file</name>
  <files>api/endpoints/jobs_endpoints.R</files>
  <action>
Create `api/endpoints/jobs_endpoints.R` with async job submission and status polling endpoints:

**File structure:**

```r
# api/endpoints/jobs_endpoints.R
#
# Async job submission and status polling endpoints.
# Uses mirai for background execution, returns HTTP 202 Accepted for long-running operations.
#
# Endpoints:
#   POST /api/jobs/clustering/submit - Submit functional clustering job
#   POST /api/jobs/phenotype_clustering/submit - Submit phenotype clustering job
#   GET /api/jobs/<job_id>/status - Poll job status and retrieve results

## -------------------------------------------------------------------##
## Job Submission Endpoints
## -------------------------------------------------------------------##
```

**1. POST /clustering/submit endpoint:**
```r
#* Submit Functional Clustering Job
#*
#* Submits an async job to compute functional clustering via STRING-db.
#* Returns immediately with job ID for status polling.
#*
#* @tag jobs
#* @serializer json list(na="string")
#* @post /clustering/submit
function(req, res) {
  # Check authentication (optional - match current API behavior)
  # Functional clustering is public in current API

  # CRITICAL: Extract request data BEFORE mirai call
  # Connection objects cannot cross process boundaries
  genes_list <- NULL
  if (!is.null(req$argsBody$genes)) {
    genes_list <- req$argsBody$genes
  }

  # If no genes provided, use default (all NDD genes)
  # This matches current functional_clustering endpoint behavior
  if (is.null(genes_list) || length(genes_list) == 0) {
    genes_list <- pool %>%
      tbl("ndd_entity_view") %>%
      arrange(entity_id) %>%
      filter(ndd_phenotype == 1) %>%
      select(hgnc_id) %>%
      collect() %>%
      unique() %>%
      pull(hgnc_id)
  }

  # Check for duplicate job
  dup_check <- check_duplicate_job("clustering", list(genes = genes_list))
  if (dup_check$duplicate) {
    res$status <- 409
    res$setHeader("Location", paste0("/api/jobs/", dup_check$existing_job_id, "/status"))
    return(list(
      error = "DUPLICATE_JOB",
      message = "Identical job already running",
      existing_job_id = dup_check$existing_job_id,
      status_url = paste0("/api/jobs/", dup_check$existing_job_id, "/status")
    ))
  }

  # Create async job
  result <- create_job(
    operation = "clustering",
    params = list(genes = genes_list),
    executor_fn = function(params) {
      # This runs in mirai daemon - use memoized version
      gen_string_clust_obj_mem(params$genes)
    }
  )

  # Check capacity
  if (!is.null(result$error)) {
    res$status <- 503
    res$setHeader("Retry-After", as.character(result$retry_after))
    return(result)
  }

  # Success - return HTTP 202 Accepted
  res$status <- 202
  res$setHeader("Location", paste0("/api/jobs/", result$job_id, "/status"))
  res$setHeader("Retry-After", "5")

  list(
    job_id = result$job_id,
    status = result$status,
    estimated_seconds = result$estimated_seconds,
    status_url = paste0("/api/jobs/", result$job_id, "/status")
  )
}
```

**2. POST /phenotype_clustering/submit endpoint:**
Similar structure but:
- operation = "phenotype_clustering"
- No genes parameter (uses fixed entity set)
- Executor function does the full data preparation (like current phenotype_clustering endpoint) then calls `gen_mca_clust_obj_mem()`
- Extract all database data BEFORE mirai call, pass as params

```r
#* Submit Phenotype Clustering Job
#*
#* Submits an async job to compute phenotype clustering via MCA.
#* Returns immediately with job ID for status polling.
#*
#* @tag jobs
#* @serializer json list(na="string")
#* @post /phenotype_clustering/submit
function(req, res) {
  # Prepare data BEFORE mirai (database connections can't cross process boundary)
  # This replicates the data gathering from phenotype_clustering endpoint

  id_phenotype_ids <- c(
    "HP:0001249", "HP:0001256", "HP:0002187",
    "HP:0002342", "HP:0006889", "HP:0010864"
  )
  categories <- c("Definitive")

  # Gather all data from database
  ndd_entity_view_tbl <- pool %>% tbl("ndd_entity_view") %>% collect()
  ndd_entity_review_tbl <- pool %>% tbl("ndd_entity_review") %>% collect() %>%
    filter(is_primary == 1) %>% select(review_id)
  ndd_review_phenotype_connect_tbl <- pool %>% tbl("ndd_review_phenotype_connect") %>% collect()
  modifier_list_tbl <- pool %>% tbl("modifier_list") %>% collect()
  phenotype_list_tbl <- pool %>% tbl("phenotype_list") %>% collect()

  # Create params hash based on entity count (stable identifier)
  params_hash_input <- list(
    entity_count = nrow(ndd_entity_view_tbl),
    operation = "phenotype_clustering"
  )

  # Check for duplicate
  dup_check <- check_duplicate_job("phenotype_clustering", params_hash_input)
  if (dup_check$duplicate) {
    res$status <- 409
    res$setHeader("Location", paste0("/api/jobs/", dup_check$existing_job_id, "/status"))
    return(list(
      error = "DUPLICATE_JOB",
      message = "Identical job already running",
      existing_job_id = dup_check$existing_job_id,
      status_url = paste0("/api/jobs/", dup_check$existing_job_id, "/status")
    ))
  }

  # Create job with pre-fetched data
  result <- create_job(
    operation = "phenotype_clustering",
    params = list(
      ndd_entity_view_tbl = ndd_entity_view_tbl,
      ndd_entity_review_tbl = ndd_entity_review_tbl,
      ndd_review_phenotype_connect_tbl = ndd_review_phenotype_connect_tbl,
      modifier_list_tbl = modifier_list_tbl,
      phenotype_list_tbl = phenotype_list_tbl,
      id_phenotype_ids = id_phenotype_ids,
      categories = categories
    ),
    executor_fn = function(params) {
      # This runs in mirai daemon
      # Replicate phenotype_clustering logic
      sysndd_db_phenotypes <- params$ndd_entity_view_tbl %>%
        left_join(params$ndd_review_phenotype_connect_tbl, by = "entity_id") %>%
        left_join(params$modifier_list_tbl, by = "modifier_id") %>%
        left_join(params$phenotype_list_tbl, by = "phenotype_id") %>%
        mutate(ndd_phenotype = case_when(
          ndd_phenotype == 1 ~ "Yes",
          ndd_phenotype == 0 ~ "No"
        )) %>%
        filter(ndd_phenotype == "Yes") %>%
        filter(category %in% params$categories) %>%
        filter(modifier_name == "present") %>%
        filter(review_id %in% params$ndd_entity_review_tbl$review_id) %>%
        select(entity_id, hpo_mode_of_inheritance_term_name, phenotype_id,
               HPO_term, hgnc_id) %>%
        group_by(entity_id) %>%
        mutate(
          phenotype_non_id_count = sum(!(phenotype_id %in% params$id_phenotype_ids)),
          phenotype_id_count = sum(phenotype_id %in% params$id_phenotype_ids)
        ) %>%
        ungroup() %>%
        unique()

      sysndd_db_phenotypes_wider <- sysndd_db_phenotypes %>%
        mutate(present = "yes") %>%
        select(-phenotype_id) %>%
        pivot_wider(names_from = HPO_term, values_from = present) %>%
        group_by(hgnc_id) %>%
        mutate(gene_entity_count = n()) %>%
        ungroup() %>%
        relocate(gene_entity_count, .after = phenotype_id_count) %>%
        select(-hgnc_id)

      sysndd_db_phenotypes_wider_df <- sysndd_db_phenotypes_wider %>%
        select(-entity_id) %>%
        as.data.frame()

      row.names(sysndd_db_phenotypes_wider_df) <- sysndd_db_phenotypes_wider$entity_id

      phenotype_clusters <- gen_mca_clust_obj_mem(sysndd_db_phenotypes_wider_df)

      # Add back identifiers
      ndd_entity_view_tbl_sub <- params$ndd_entity_view_tbl %>%
        select(entity_id, hgnc_id, symbol)

      phenotype_clusters %>%
        unnest(identifiers) %>%
        mutate(entity_id = as.integer(entity_id)) %>%
        left_join(ndd_entity_view_tbl_sub, by = "entity_id") %>%
        nest(identifiers = c(entity_id, hgnc_id, symbol))
    }
  )

  if (!is.null(result$error)) {
    res$status <- 503
    res$setHeader("Retry-After", as.character(result$retry_after))
    return(result)
  }

  res$status <- 202
  res$setHeader("Location", paste0("/api/jobs/", result$job_id, "/status"))
  res$setHeader("Retry-After", "5")

  list(
    job_id = result$job_id,
    status = result$status,
    estimated_seconds = 60,  # MCA typically longer
    status_url = paste0("/api/jobs/", result$job_id, "/status")
  )
}
```

**3. GET /<job_id>/status endpoint:**
```r
#* Get Job Status
#*
#* Poll job status and retrieve results when complete.
#* Returns Retry-After header for running jobs.
#*
#* @tag jobs
#* @serializer json list(na="string")
#* @get /<job_id>/status
function(job_id, res) {
  status <- get_job_status(job_id)

  if (!is.null(status$error) && status$error == "JOB_NOT_FOUND") {
    res$status <- 404
    return(list(
      error = "JOB_NOT_FOUND",
      message = paste0("Job '", job_id, "' not found or expired")
    ))
  }

  # Set Retry-After for running jobs
  if (status$status %in% c("pending", "running")) {
    res$setHeader("Retry-After", as.character(status$retry_after %||% 5))
  }

  res$status <- 200
  return(status)
}
```

Add file header with roxygen and proper comment sections.
  </action>
  <verify>
Run syntax check: `Rscript -e "parse('endpoints/jobs_endpoints.R'); cat('Syntax OK\n')"` from api directory.
  </verify>
  <done>
jobs_endpoints.R exists with clustering/submit, phenotype_clustering/submit, and status polling endpoints.
  </done>
</task>

<task type="auto">
  <name>Task 2: Mount jobs endpoints and update analysis endpoints</name>
  <files>api/start_sysndd_api.R, api/endpoints/analysis_endpoints.R</files>
  <action>
1. **Modify start_sysndd_api.R:**

   Add jobs endpoints mount after analysis endpoints mount (~line 347):
   ```r
   pr_mount("/api/jobs", pr("endpoints/jobs_endpoints.R")) %>%
   ```

   Full context - it should appear in the mount chain:
   ```r
   pr_mount("/api/analysis", pr("endpoints/analysis_endpoints.R")) %>%
   pr_mount("/api/jobs", pr("endpoints/jobs_endpoints.R")) %>%
   pr_mount("/api/hash", pr("endpoints/hash_endpoints.R")) %>%
   ```

2. **Update analysis_endpoints.R:**

   Add comments to existing sync endpoints noting the async alternatives:

   At the top of the file, add a note:
   ```r
   # Note: For long-running operations, consider using async endpoints:
   #   POST /api/jobs/clustering/submit - Async functional clustering
   #   POST /api/jobs/phenotype_clustering/submit - Async phenotype clustering
   #   GET /api/jobs/<job_id>/status - Poll job status
   ```

   Keep existing sync endpoints unchanged for backward compatibility.
   They still work (using memoized functions), but new clients should use async for large datasets.

3. **Verify required globals are available:**

   The jobs_endpoints.R needs access to:
   - `pool` (global, already available)
   - `create_job`, `get_job_status`, `check_duplicate_job` (from job-manager.R, sourced in start_sysndd_api.R)
   - `gen_string_clust_obj_mem`, `gen_mca_clust_obj_mem` (memoized functions, already global)

   These should all be available since start_sysndd_api.R sources job-manager.R before creating the plumber router.
  </action>
  <verify>
1. Syntax check: `Rscript -e "parse('start_sysndd_api.R'); cat('OK\n')"` from api directory
2. Verify mount: `grep -n "jobs_endpoints" start_sysndd_api.R`
3. Check analysis endpoints note: `head -20 endpoints/analysis_endpoints.R`
  </verify>
  <done>
- /api/jobs endpoints mounted in start_sysndd_api.R
- analysis_endpoints.R has documentation note about async alternatives
- All syntax checks pass
  </done>
</task>

</tasks>

<verification>
1. Syntax verification for all modified files:
   ```bash
   cd /home/bernt-popp/development/sysndd/api
   Rscript -e "parse('endpoints/jobs_endpoints.R'); parse('endpoints/analysis_endpoints.R'); parse('start_sysndd_api.R'); cat('All files parse OK\n')"
   ```

2. Verify jobs mount exists:
   ```bash
   grep -n "api/jobs" /home/bernt-popp/development/sysndd/api/start_sysndd_api.R
   ```

3. Verify endpoint structure in jobs_endpoints.R:
   ```bash
   grep -E "@(post|get)" /home/bernt-popp/development/sysndd/api/endpoints/jobs_endpoints.R
   ```
   Should show: @post /clustering/submit, @post /phenotype_clustering/submit, @get /<job_id>/status
</verification>

<success_criteria>
- api/endpoints/jobs_endpoints.R created with 3 endpoints (2 submit, 1 status)
- POST /api/jobs/clustering/submit returns HTTP 202 structure
- POST /api/jobs/phenotype_clustering/submit returns HTTP 202 structure
- GET /api/jobs/<job_id>/status returns job state with Retry-After header
- Duplicate detection returns HTTP 409 with Location header
- Jobs endpoints mounted at /api/jobs in start_sysndd_api.R
- All R files parse without syntax errors
</success_criteria>

<output>
After completion, create `.planning/phases/20-async-non-blocking/20-02-SUMMARY.md`
</output>
