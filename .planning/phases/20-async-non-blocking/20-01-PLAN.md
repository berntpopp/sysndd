# Phase 20: Async/Non-blocking - Plan 01

---
phase: 20-async-non-blocking
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - api/functions/job-manager.R
  - api/start_sysndd_api.R
  - renv.lock
autonomous: true

must_haves:
  truths:
    - "mirai daemon pool initializes on API startup"
    - "Job manager can create, track, and query job state"
    - "New packages (mirai, promises, uuid) are installed and loadable"
  artifacts:
    - path: "api/functions/job-manager.R"
      provides: "Job state management functions"
      exports: ["create_job", "get_job_status", "check_duplicate_job", "cleanup_old_jobs", "jobs_env", "MAX_CONCURRENT_JOBS"]
    - path: "api/start_sysndd_api.R"
      provides: "Daemon pool initialization"
      contains: "daemons("
  key_links:
    - from: "api/start_sysndd_api.R"
      to: "api/functions/job-manager.R"
      via: "source() call"
      pattern: 'source\\("functions/job-manager\\.R"'
    - from: "api/start_sysndd_api.R"
      to: "mirai daemons"
      via: "daemons() initialization"
      pattern: "daemons\\(.*n\\s*=\\s*8"
---

<objective>
Create the core async infrastructure for long-running API operations.

Purpose: Establish the foundation that enables endpoints to execute tasks asynchronously without blocking the main API thread. This is the prerequisite for converting clustering and ontology endpoints to async in subsequent plans.

Output: Job manager module with state management functions, mirai daemon pool initialization in API startup, and new package dependencies installed.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/20-async-non-blocking/20-CONTEXT.md
@.planning/phases/20-async-non-blocking/20-RESEARCH.md

# Existing files to modify
@api/start_sysndd_api.R
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create job manager module</name>
  <files>api/functions/job-manager.R</files>
  <action>
Create `api/functions/job-manager.R` with job state management functions based on RESEARCH.md patterns:

1. **Global state:**
   - `jobs_env <- new.env(parent = emptyenv())` for job storage
   - `MAX_CONCURRENT_JOBS <- 8` (matches daemon pool size)

2. **create_job(operation, params, executor_fn):**
   - Check capacity (count running jobs, return error if >= MAX_CONCURRENT_JOBS)
   - Generate UUID via `uuid::UUIDgenerate()`
   - Create mirai task with `.timeout = 1800000` (30 min in ms)
   - Store job state in `jobs_env[[job_id]]` with: job_id, operation, status ("pending"), mirai_obj, submitted_at, params_hash (via `digest::digest()`), result (NULL), error (NULL), completed_at (NULL)
   - Attach promise callback via `%...>%` to update status on completion
   - Check `is_mirai_error()` and `is_error_value()` before storing result
   - Return list(job_id, status="accepted", estimated_seconds=30) on success
   - Return list(error="CAPACITY_EXCEEDED", message, retry_after=60) on capacity exceeded

3. **get_job_status(job_id):**
   - Return list(error="JOB_NOT_FOUND") if job doesn't exist
   - Check `unresolved(m)` for running status
   - For running jobs: return status, step (from get_progress_message), estimated_seconds, retry_after=5
   - For completed jobs: return cached status, completed_at, result or error from job state

4. **get_progress_message(operation):**
   - Return operation-specific progress messages:
     - "clustering" -> "Fetching interaction data from STRING-db..."
     - "phenotype_clustering" -> "Running Multiple Correspondence Analysis..."
     - "ontology_update" -> "Fetching ontology data from external sources..."
     - Default -> "Processing request..."

5. **check_duplicate_job(operation, params):**
   - Hash params with `digest::digest()`
   - Scan `jobs_env` for matching operation + params_hash with status in c("pending", "running")
   - Return list(duplicate=TRUE, existing_job_id) if found
   - Return list(duplicate=FALSE) otherwise

6. **cleanup_old_jobs():**
   - Calculate cutoff time: `Sys.time() - (24 * 3600)` (24 hours)
   - For each job in jobs_env with status "completed" or "failed":
     - If completed_at (or submitted_at fallback) < cutoff: remove from jobs_env
   - Log cleanup count via `message()`

Include roxygen-style documentation for each function.

Use `%||%` from rlang for null coalescing (already loaded in start_sysndd_api.R).
  </action>
  <verify>
Run `Rscript -e "source('api/functions/job-manager.R'); cat('Job manager loads OK\n')"` from api directory.
Verify no syntax errors.
  </verify>
  <done>
job-manager.R exists with all 6 required functions documented and syntactically correct.
  </done>
</task>

<task type="auto">
  <name>Task 2: Add package dependencies and initialize daemon pool</name>
  <files>api/start_sysndd_api.R, renv.lock</files>
  <action>
1. **Add packages to renv:**
   Run in api directory:
   ```r
   renv::install(c("mirai", "promises", "uuid", "digest"))
   renv::snapshot()
   ```

   Note: `digest` is likely already installed but ensure it's captured. `later` is a dependency of plumber, should already be present.

2. **Modify start_sysndd_api.R:**

   a) Add library calls after existing library block (around line 65):
   ```r
   library(mirai)
   library(promises)
   library(uuid)
   ```

   b) Add source for job-manager.R in section 4 (after line 118, with other function sources):
   ```r
   source("functions/job-manager.R", local = TRUE)
   ```

   c) Add daemon pool initialization after section 8 (after memoization, before filters ~line 199). Create new section:
   ```r
   ## -------------------------------------------------------------------##
   # 9.5) Initialize mirai daemon pool for async jobs
   ## -------------------------------------------------------------------##
   daemons(
     n = 8,              # 8 workers for concurrent jobs
     dispatcher = TRUE,  # Enable for variable-length jobs
     autoexit = tools::SIGINT
   )
   message(sprintf("[%s] Started mirai daemon pool with 8 workers", Sys.time()))
   ```

   d) Modify the exit hook (cleanupHook function, ~line 282) to also shutdown daemons:
   ```r
   cleanupHook <- function(pr) {
     pr %>%
       pr_hook("exit", function() {
         pool::poolClose(pool)
         message("Disconnected from DB")
         daemons(0)  # Shutdown mirai daemon pool
         message("Shutdown mirai daemon pool")
       })
   }
   ```

   e) Add background cleanup scheduling after daemon initialization (use `later` package which is already a plumber dependency):
   ```r
   # Schedule hourly job cleanup
   later::later(function() {
     cleanup_old_jobs()
     # Re-schedule (later doesn't have loop=TRUE, need recursive call)
   }, 3600)
   ```

   Note: For proper recurring cleanup, we'll implement a self-scheduling pattern in job-manager.R's cleanup function or use a simpler approach in the jobs endpoint file in Plan 02.
  </action>
  <verify>
1. Run `cd /home/bernt-popp/development/sysndd/api && Rscript -e "library(mirai); library(promises); library(uuid); cat('All packages load OK\n')"`
2. Verify renv.lock contains mirai, promises, uuid entries
3. Syntax check: `Rscript -e "parse('start_sysndd_api.R'); cat('Syntax OK\n')"`
  </verify>
  <done>
- mirai, promises, uuid packages installed and in renv.lock
- start_sysndd_api.R loads job-manager.R, initializes daemon pool with 8 workers
- Exit hook shuts down daemon pool
  </done>
</task>

</tasks>

<verification>
1. Package verification:
   ```bash
   cd /home/bernt-popp/development/sysndd/api
   Rscript -e "library(mirai); library(promises); library(uuid); cat('Packages OK\n')"
   ```

2. Job manager syntax:
   ```bash
   cd /home/bernt-popp/development/sysndd/api
   Rscript -e "source('functions/job-manager.R'); ls(pattern='job|cleanup', envir=globalenv())"
   ```
   Should show: cleanup_old_jobs, create_job, get_job_status, check_duplicate_job

3. Start script syntax:
   ```bash
   cd /home/bernt-popp/development/sysndd/api
   Rscript -e "parse('start_sysndd_api.R'); cat('Start script parses OK\n')"
   ```

4. Verify renv.lock contains new packages:
   ```bash
   grep -E '"mirai"|"promises"|"uuid"' /home/bernt-popp/development/sysndd/api/renv.lock
   ```
</verification>

<success_criteria>
- api/functions/job-manager.R exists with create_job, get_job_status, check_duplicate_job, cleanup_old_jobs, get_progress_message functions
- mirai, promises, uuid packages in renv.lock
- start_sysndd_api.R sources job-manager.R
- start_sysndd_api.R initializes 8 mirai daemons with dispatcher=TRUE
- Exit hook closes both pool and daemons
- All files parse without syntax errors
</success_criteria>

<output>
After completion, create `.planning/phases/20-async-non-blocking/20-01-SUMMARY.md`
</output>
