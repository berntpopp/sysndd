# Phase 20: Async/Non-blocking - Plan 03

---
phase: 20-async-non-blocking
plan: 03
type: execute
wave: 3
depends_on: ["20-02"]
files_modified:
  - api/endpoints/jobs_endpoints.R
  - api/functions/job-manager.R
autonomous: false

must_haves:
  truths:
    - "Ontology update job submission returns HTTP 202 with job ID immediately"
    - "Background cleanup removes jobs older than 24 hours"
    - "All async endpoints respond without blocking other API requests"
    - "Job status polling works for all three job types"
  artifacts:
    - path: "api/endpoints/jobs_endpoints.R"
      provides: "Ontology update async endpoint"
      contains: "ontology_update/submit"
    - path: "api/functions/job-manager.R"
      provides: "Self-scheduling cleanup function"
      contains: "schedule_cleanup"
  key_links:
    - from: "api/endpoints/jobs_endpoints.R"
      to: "api/functions/ontology-functions.R"
      via: "process_combine_ontology() call"
      pattern: "process_combine_ontology"
---

<objective>
Add async ontology update endpoint and implement background job cleanup with integration verification.

Purpose: Complete the async infrastructure by adding the ontology update endpoint (the other major long-running operation) and implementing the background cleanup sweep to prevent memory leaks. Verify the complete async system works end-to-end.

Output: Ontology update async endpoint, self-scheduling cleanup function, verified async operation without blocking.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/20-async-non-blocking/20-CONTEXT.md
@.planning/phases/20-async-non-blocking/20-RESEARCH.md
@.planning/phases/20-async-non-blocking/20-01-SUMMARY.md
@.planning/phases/20-async-non-blocking/20-02-SUMMARY.md

# Files to reference
@api/endpoints/jobs_endpoints.R
@api/endpoints/ontology_endpoints.R
@api/functions/job-manager.R
@api/functions/ontology-functions.R
@api/start_sysndd_api.R
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add ontology update async endpoint</name>
  <files>api/endpoints/jobs_endpoints.R</files>
  <action>
Add the ontology update async endpoint to jobs_endpoints.R:

**POST /ontology_update/submit endpoint:**

This endpoint triggers the `process_combine_ontology()` function which:
- Downloads and processes MONDO ontology
- Downloads and processes OMIM genemap2 file
- Combines them with mappings
- Saves results to CSV files

The operation requires database access for HGNC list and mode of inheritance list, plus external HTTP downloads. Both data and downloads must be handled carefully for async:

```r
#* Submit Ontology Update Job
#*
#* Submits an async job to update disease ontology data from MONDO and OMIM sources.
#* Requires Administrator role.
#* Returns immediately with job ID for status polling.
#*
#* @tag jobs
#* @serializer json list(na="string")
#* @post /ontology_update/submit
function(req, res) {
  # Check authentication - require Administrator
  if (is.null(req$user_id)) {
    res$status <- 401
    return(list(error = "UNAUTHORIZED", message = "Authentication required"))
  }

  if (req$user_role != "Administrator") {
    res$status <- 403
    return(list(error = "FORBIDDEN", message = "Administrator role required"))
  }

  # CRITICAL: Extract all database data BEFORE mirai
  # Database connections cannot cross process boundaries

  # Get HGNC list
  hgnc_list <- pool %>%
    tbl("non_alt_loci_set") %>%
    select(symbol, hgnc_id) %>%
    collect()

  # Get mode of inheritance list
  mode_of_inheritance_list <- pool %>%
    tbl("mode_of_inheritance_list") %>%
    filter(is_active == 1) %>%
    select(hpo_mode_of_inheritance_term, hpo_mode_of_inheritance_term_name) %>%
    collect()

  # Check for duplicate job (ontology update has no params variation)
  dup_check <- check_duplicate_job("ontology_update", list(operation = "ontology_update"))
  if (dup_check$duplicate) {
    res$status <- 409
    res$setHeader("Location", paste0("/api/jobs/", dup_check$existing_job_id, "/status"))
    return(list(
      error = "DUPLICATE_JOB",
      message = "Ontology update job already running",
      existing_job_id = dup_check$existing_job_id,
      status_url = paste0("/api/jobs/", dup_check$existing_job_id, "/status")
    ))
  }

  # Create async job
  result <- create_job(
    operation = "ontology_update",
    params = list(
      hgnc_list = hgnc_list,
      mode_of_inheritance_list = mode_of_inheritance_list
    ),
    executor_fn = function(params) {
      # This runs in mirai daemon
      # The process_combine_ontology function handles:
      # - Downloading MONDO ontology
      # - Downloading OMIM genemap2
      # - Processing and combining data
      # - Saving results to CSV

      # Call the ontology processing function
      disease_ontology_set <- process_combine_ontology(
        hgnc_list = params$hgnc_list,
        mode_of_inheritance_list = params$mode_of_inheritance_list,
        max_file_age = 0,  # Force regeneration
        output_path = "data/"
      )

      # Return summary
      list(
        status = "completed",
        rows_processed = nrow(disease_ontology_set),
        sources = c("MONDO", "OMIM"),
        output_file = paste0("data/disease_ontology_set.", format(Sys.Date(), "%Y-%m-%d"), ".csv")
      )
    }
  )

  # Check capacity
  if (!is.null(result$error)) {
    res$status <- 503
    res$setHeader("Retry-After", as.character(result$retry_after))
    return(result)
  }

  # Success - return HTTP 202 Accepted
  res$status <- 202
  res$setHeader("Location", paste0("/api/jobs/", result$job_id, "/status"))
  res$setHeader("Retry-After", "30")  # Longer polling interval for ontology update

  list(
    job_id = result$job_id,
    status = result$status,
    estimated_seconds = 300,  # Ontology update is slow (5+ minutes)
    status_url = paste0("/api/jobs/", result$job_id, "/status")
  )
}
```

Also update the progress message in job-manager.R if needed (Task 2 will handle this).
  </action>
  <verify>
Syntax check: `Rscript -e "parse('endpoints/jobs_endpoints.R'); cat('Syntax OK\n')"` from api directory.
Verify endpoint exists: `grep -n "ontology_update/submit" endpoints/jobs_endpoints.R`
  </verify>
  <done>
POST /api/jobs/ontology_update/submit endpoint added with Administrator auth check.
  </done>
</task>

<task type="auto">
  <name>Task 2: Implement self-scheduling cleanup and update progress messages</name>
  <files>api/functions/job-manager.R</files>
  <action>
Update job-manager.R to add:

1. **Update get_progress_message()** to include ontology_update:
```r
get_progress_message <- function(operation) {
  messages <- list(
    clustering = "Fetching interaction data from STRING-db...",
    phenotype_clustering = "Running Multiple Correspondence Analysis...",
    ontology_update = "Downloading and processing ontology data from MONDO/OMIM..."
  )
  messages[[operation]] %||% "Processing request..."
}
```

2. **Add schedule_cleanup() function** for self-scheduling cleanup:
```r
#' Schedule Recurring Job Cleanup
#'
#' Schedules the cleanup_old_jobs function to run periodically.
#' Uses `later` package for non-blocking scheduling.
#' Default interval is 1 hour (3600 seconds).
#'
#' @param interval_seconds Interval between cleanup runs in seconds
#' @export
schedule_cleanup <- function(interval_seconds = 3600) {
  cleanup_and_reschedule <- function() {
    cleanup_old_jobs()
    # Reschedule
    later::later(cleanup_and_reschedule, interval_seconds)
  }

  # Start the first cleanup cycle
  later::later(cleanup_and_reschedule, interval_seconds)
  message(sprintf("[%s] Scheduled job cleanup every %d seconds", Sys.time(), interval_seconds))
}
```

3. **Update cleanup_old_jobs()** to handle edge cases:
```r
#' Clean Up Old Jobs
#'
#' Removes completed and failed jobs older than 24 hours from jobs_env.
#' Called periodically by schedule_cleanup().
#'
#' @export
cleanup_old_jobs <- function() {
  cutoff_time <- Sys.time() - (24 * 3600)  # 24 hours ago
  removed <- 0

  job_ids <- ls(jobs_env)

  if (length(job_ids) == 0) {
    return(invisible(0))
  }

  for (job_id in job_ids) {
    tryCatch({
      job <- jobs_env[[job_id]]

      if (is.null(job)) {
        # Orphaned entry, remove it
        rm(list = job_id, envir = jobs_env)
        removed <- removed + 1
        next
      }

      if (job$status %in% c("completed", "failed")) {
        end_time <- job$completed_at %||% job$submitted_at

        if (!is.null(end_time) && end_time < cutoff_time) {
          rm(list = job_id, envir = jobs_env)
          removed <- removed + 1
        }
      }
    }, error = function(e) {
      message(sprintf("[%s] Error cleaning job %s: %s", Sys.time(), job_id, e$message))
    })
  }

  if (removed > 0) {
    message(sprintf("[%s] Cleaned up %d old jobs", Sys.time(), removed))
  }

  invisible(removed)
}
```

4. **Update start_sysndd_api.R** to call schedule_cleanup():

After daemon initialization, add:
```r
# Schedule hourly job cleanup
schedule_cleanup(3600)  # 3600 seconds = 1 hour
```

This replaces the simpler `later::later()` call from Plan 01.
  </action>
  <verify>
1. Syntax check: `Rscript -e "parse('functions/job-manager.R'); cat('Syntax OK\n')"` from api directory
2. Verify functions exist: `grep -E "^(cleanup_old_jobs|schedule_cleanup|get_progress_message)" functions/job-manager.R`
3. Verify ontology_update message: `grep "ontology_update" functions/job-manager.R`
  </verify>
  <done>
- get_progress_message includes ontology_update
- schedule_cleanup function implements self-scheduling pattern
- cleanup_old_jobs handles edge cases with tryCatch
- start_sysndd_api.R calls schedule_cleanup(3600)
  </done>
</task>

<task type="checkpoint:human-verify" gate="blocking">
  <what-built>
Complete async job infrastructure:
- mirai daemon pool (8 workers) initialized on API startup
- Job manager with state tracking
- Three async endpoints: /api/jobs/clustering/submit, /api/jobs/phenotype_clustering/submit, /api/jobs/ontology_update/submit
- Job status polling at /api/jobs/{job_id}/status
- HTTP 202 Accepted with Location and Retry-After headers
- HTTP 409 Conflict for duplicate jobs
- Background cleanup every hour
  </what-built>
  <how-to-verify>
1. Start the API:
   ```bash
   cd /home/bernt-popp/development/sysndd
   docker compose up api --build
   ```
   Watch logs for: "Started mirai daemon pool with 8 workers"

2. Test async clustering submission (returns immediately):
   ```bash
   curl -X POST http://localhost:3000/api/jobs/clustering/submit \
     -H "Content-Type: application/json" \
     -d '{"genes": ["HGNC:1", "HGNC:2", "HGNC:3"]}' \
     -i
   ```
   Expected: HTTP 202 Accepted with Location header pointing to status URL

3. Test job status polling (use job_id from step 2):
   ```bash
   curl http://localhost:3000/api/jobs/{JOB_ID}/status -i
   ```
   Expected: JSON with status (pending/running/completed/failed), Retry-After header if running

4. Test duplicate detection:
   ```bash
   # Submit same job twice quickly
   curl -X POST http://localhost:3000/api/jobs/clustering/submit \
     -d '{"genes": ["HGNC:1"]}' -i
   curl -X POST http://localhost:3000/api/jobs/clustering/submit \
     -d '{"genes": ["HGNC:1"]}' -i
   ```
   Expected: Second request returns HTTP 409 Conflict

5. Verify non-blocking (while job running, other endpoints respond):
   ```bash
   # While a job is running:
   curl http://localhost:3000/health
   ```
   Expected: Immediate response, not blocked by running job

6. Test original sync endpoints still work:
   ```bash
   curl http://localhost:3000/api/analysis/functional_clustering | head -c 500
   ```
   Expected: Returns clustering data (may take time, but works)
  </how-to-verify>
  <resume-signal>Type "approved" if all tests pass, or describe any issues encountered</resume-signal>
</task>

</tasks>

<verification>
1. All R files parse:
   ```bash
   cd /home/bernt-popp/development/sysndd/api
   Rscript -e "parse('functions/job-manager.R'); parse('endpoints/jobs_endpoints.R'); parse('start_sysndd_api.R'); cat('All files parse OK\n')"
   ```

2. All three job submission endpoints exist:
   ```bash
   grep -E "@post.*/submit" /home/bernt-popp/development/sysndd/api/endpoints/jobs_endpoints.R
   ```

3. Schedule cleanup exists:
   ```bash
   grep "schedule_cleanup" /home/bernt-popp/development/sysndd/api/functions/job-manager.R
   grep "schedule_cleanup" /home/bernt-popp/development/sysndd/api/start_sysndd_api.R
   ```
</verification>

<success_criteria>
- POST /api/jobs/ontology_update/submit requires Administrator auth and returns HTTP 202
- get_progress_message returns appropriate message for ontology_update
- schedule_cleanup implements self-scheduling hourly cleanup
- cleanup_old_jobs handles edge cases safely
- Human verification confirms:
  - Daemon pool starts (logs show 8 workers)
  - Async submission returns HTTP 202 immediately
  - Status polling returns job state with headers
  - Duplicate detection returns HTTP 409
  - Other endpoints respond while jobs run (non-blocking)
  - Original sync endpoints still function
</success_criteria>

<output>
After completion, create `.planning/phases/20-async-non-blocking/20-03-SUMMARY.md`
</output>
