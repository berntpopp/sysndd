---
phase: 58-llm-foundation
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - api/functions/llm-service.R
  - api/functions/llm-cache-repository.R
  - db/migrations/006_add_llm_summary_cache.sql
  - api/renv.lock
autonomous: true

must_haves:
  truths:
    - "ellmer package is installed and loadable"
    - "GEMINI_API_KEY environment variable is read by LLM client"
    - "Gemini API calls return structured JSON with defined schema"
    - "LLM cache tables exist in database with correct schema"
    - "Generation logs capture all API call attempts"
  artifacts:
    - path: "api/functions/llm-service.R"
      provides: "Gemini API client with ellmer"
      exports: ["generate_cluster_summary", "build_cluster_prompt", "cluster_summary_type"]
    - path: "api/functions/llm-cache-repository.R"
      provides: "Database cache operations"
      exports: ["generate_cluster_hash", "get_cached_summary", "save_summary_to_cache", "log_generation_attempt"]
    - path: "db/migrations/006_add_llm_summary_cache.sql"
      provides: "LLM cache schema"
      contains: "llm_cluster_summary_cache"
  key_links:
    - from: "api/functions/llm-service.R"
      to: "ellmer::chat_google_gemini"
      via: "chat$chat_structured()"
      pattern: "chat_google_gemini|chat_structured"
    - from: "api/functions/llm-service.R"
      to: "api/functions/llm-cache-repository.R"
      via: "source and function calls"
      pattern: "log_generation_attempt|save_summary"
    - from: "api/functions/llm-cache-repository.R"
      to: "db_execute_query"
      via: "database helper functions"
      pattern: "db_execute_query|db_execute_statement"
---

<objective>
Create Gemini API client using ellmer package with structured JSON output schema and database storage infrastructure.

Purpose: Establish the foundation for generating AI-powered cluster summaries with guaranteed structured output and complete audit logging.

Output: LLM service functions, cache repository, and database migration ready for entity validation (Plan 02).
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/58-llm-foundation/58-CONTEXT.md
@.planning/phases/58-llm-foundation/58-RESEARCH.md
@api/functions/external-proxy-functions.R
@api/functions/pubtator-functions.R
@db/migrations/005_add_pubtator_gene_symbols.sql
@api/functions/analyses-functions.R
</context>

<tasks>

<task type="auto">
  <name>Task 1: Database migration for LLM cache tables</name>
  <files>db/migrations/006_add_llm_summary_cache.sql</files>
  <action>
Create idempotent migration following 005_add_pubtator_gene_symbols.sql pattern:

1. Create `llm_cluster_summary_cache` table:
   - `cache_id` INT AUTO_INCREMENT PRIMARY KEY
   - `cluster_type` ENUM('functional', 'phenotype') NOT NULL
   - `cluster_number` INT NOT NULL
   - `cluster_hash` VARCHAR(64) NOT NULL - SHA256 of sorted gene/entity IDs
   - `model_name` VARCHAR(50) NOT NULL
   - `prompt_version` VARCHAR(20) NOT NULL DEFAULT '1.0'
   - `summary_json` JSON NOT NULL - full structured LLM response
   - `tags` JSON - extracted tags for search/filtering
   - `is_current` BOOLEAN NOT NULL DEFAULT TRUE
   - `validation_status` ENUM('pending', 'validated', 'rejected') NOT NULL DEFAULT 'pending'
   - `created_at` TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP
   - `validated_at` TIMESTAMP NULL
   - `validated_by` INT NULL - user_id of validator

2. Create `llm_generation_log` table:
   - `log_id` INT AUTO_INCREMENT PRIMARY KEY
   - `cluster_type` ENUM('functional', 'phenotype') NOT NULL
   - `cluster_number` INT NOT NULL
   - `cluster_hash` VARCHAR(64) NOT NULL
   - `model_name` VARCHAR(50) NOT NULL
   - `prompt_text` TEXT NOT NULL
   - `response_json` JSON - raw LLM response (success or partial)
   - `validation_errors` TEXT - validation failure details
   - `tokens_input` INT
   - `tokens_output` INT
   - `latency_ms` INT
   - `status` ENUM('success', 'validation_failed', 'api_error', 'timeout') NOT NULL
   - `error_message` TEXT
   - `created_at` TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP

3. Add indexes:
   - idx_cluster_hash on both tables
   - idx_cluster_type_number, idx_validation_status, idx_is_current on cache table
   - idx_status, idx_created_at on log table
   - Multi-valued index on tags JSON array (MySQL 8.0.17+)

Use stored procedure pattern with IF NOT EXISTS checks for idempotency.
  </action>
  <verify>
Run migration manually against test database:
```bash
cd /home/bernt-popp/development/sysndd
make docker-dev-db  # Start test database if not running
mysql -h 127.0.0.1 -P 7655 -u root -ptest_password sysndd_test < db/migrations/006_add_llm_summary_cache.sql
mysql -h 127.0.0.1 -P 7655 -u root -ptest_password sysndd_test -e "SHOW TABLES LIKE 'llm%';"
```
Both tables should appear.
  </verify>
  <done>
Migration file exists and creates both llm_cluster_summary_cache and llm_generation_log tables idempotently.
  </done>
</task>

<task type="auto">
  <name>Task 2: LLM cache repository functions</name>
  <files>api/functions/llm-cache-repository.R</files>
  <action>
Create `api/functions/llm-cache-repository.R` following pubtator-functions.R patterns:

1. Load dependencies:
   - require(tidyverse), require(jsonlite), require(logger), require(DBI), require(digest)
   - Source db-helpers.R if not loaded

2. Implement `generate_cluster_hash(identifiers, cluster_type)`:
   - For "functional": sort hgnc_ids, join with comma, SHA256 hash
   - For "phenotype": sort entity_ids, join with comma, SHA256 hash
   - Use digest::digest(algo = "sha256", serialize = FALSE)

3. Implement `get_cached_summary(cluster_hash)`:
   - Query llm_cluster_summary_cache WHERE cluster_hash = ? AND is_current = TRUE
   - Return tibble or NULL if not found
   - Include validation_status in return

4. Implement `save_summary_to_cache(cluster_type, cluster_number, cluster_hash, model_name, prompt_version, summary_json, tags)`:
   - Mark any existing current summaries for this cluster as is_current = FALSE
   - INSERT new row with is_current = TRUE, validation_status = 'pending'
   - Use db_with_transaction() for atomicity
   - Return cache_id of inserted row

5. Implement `log_generation_attempt(cluster_type, cluster_number, cluster_hash, model_name, prompt_text, response_json, validation_errors, tokens_input, tokens_output, latency_ms, status, error_message)`:
   - INSERT into llm_generation_log
   - All parameters optional except cluster_type, cluster_number, cluster_hash, model_name, status
   - Use db_execute_statement()

Add roxygen2 documentation for all exported functions.
  </action>
  <verify>
Source the file in R and verify functions exist:
```r
source("api/functions/llm-cache-repository.R")
exists("generate_cluster_hash")
exists("get_cached_summary")
exists("save_summary_to_cache")
exists("log_generation_attempt")
```
All should return TRUE.
  </verify>
  <done>
llm-cache-repository.R exports generate_cluster_hash, get_cached_summary, save_summary_to_cache, and log_generation_attempt with full documentation.
  </done>
</task>

<task type="auto">
  <name>Task 3: Gemini API client with ellmer</name>
  <files>api/functions/llm-service.R, api/renv.lock</files>
  <action>
1. Add ellmer to renv:
   ```bash
   cd /home/bernt-popp/development/sysndd/api
   R -e "renv::install('ellmer'); renv::snapshot()"
   ```

2. Create `api/functions/llm-service.R` following external-proxy-functions.R patterns:

   a) Load dependencies:
      - require(ellmer), require(glue), require(logger), require(jsonlite)
      - Source llm-cache-repository.R

   b) Define rate limit config:
      ```r
      GEMINI_RATE_LIMIT <- list(
        capacity = 30,      # Conservative: 30 RPM (Paid Tier 1 is higher)
        fill_time_s = 60
      )
      ```

   c) Define type specifications using ellmer's type_object():
      - `functional_cluster_summary_type` with:
        - summary (type_string, 2-3 sentences)
        - key_themes (type_array of type_string, 3-5 themes)
        - pathways (type_array of type_string, from enrichment data)
        - tags (type_array of type_string, 3-7 searchable tags)
        - clinical_relevance (type_string, required = FALSE)
        - confidence (type_enum: "high", "medium", "low")
      - `phenotype_cluster_summary_type` extends functional with:
        - syndrome_hints (type_array, required = FALSE)
        - curation_notes (type_string, required = FALSE)

   d) Implement `build_cluster_prompt(cluster_data, top_n_terms = 20)`:
      - Extract gene symbols from cluster_data$identifiers
      - Format top enrichment terms by category from cluster_data$term_enrichment
      - Return glue template with cluster info and instructions
      - Do NOT include JSON schema in prompt text (ellmer handles via type spec)

   e) Implement `generate_cluster_summary(cluster_data, cluster_type = "functional", model = "gemini-3-pro-preview", max_retries = 3)`:
      - Check GEMINI_API_KEY environment variable, error if missing
      - Build prompt using build_cluster_prompt()
      - Create chat with ellmer::chat_google_gemini(model = model)
      - Use chat$chat_structured() with appropriate type spec
      - Implement retry with exponential backoff + jitter (2^retries + runif(0,1))
      - Log all attempts via log_generation_attempt()
      - Return list(success, summary, tokens_used, latency_ms) or list(success = FALSE, error, attempts)

   f) Implement `get_or_generate_summary(cluster_data, cluster_type = "functional", model = "gemini-3-pro-preview")`:
      - Generate cluster hash
      - Check cache with get_cached_summary()
      - If cached and validated, return from cache
      - Otherwise generate new, save to cache (validation_status = 'pending')
      - Return summary with metadata (from_cache, validation_status)

Add roxygen2 documentation for all exported functions.

IMPORTANT: Do NOT hardcode API key. Read from Sys.getenv("GEMINI_API_KEY").
  </action>
  <verify>
1. Check ellmer is installed:
   ```bash
   cd /home/bernt-popp/development/sysndd/api
   R -e "library(ellmer); packageVersion('ellmer')"
   ```
   Should show version >= 0.4.0

2. Source and verify functions:
   ```r
   source("api/functions/llm-service.R")
   exists("generate_cluster_summary")
   exists("build_cluster_prompt")
   exists("functional_cluster_summary_type")
   ```
   All should return TRUE.

3. Verify API key check:
   ```r
   # Should error with clear message
   tryCatch(
     generate_cluster_summary(list(), "functional"),
     error = function(e) grepl("GEMINI_API_KEY", e$message)
   )
   ```
   Should return TRUE.
  </verify>
  <done>
ellmer >= 0.4.0 installed, llm-service.R exports type specifications, build_cluster_prompt, generate_cluster_summary, and get_or_generate_summary with proper GEMINI_API_KEY handling.
  </done>
</task>

</tasks>

<verification>
1. Migration creates tables: `mysql ... -e "DESCRIBE llm_cluster_summary_cache; DESCRIBE llm_generation_log;"`
2. R files load without error: `R -e "source('api/functions/llm-service.R')"`
3. ellmer is in renv.lock: `grep -A2 '"ellmer"' api/renv.lock`
4. No hardcoded API keys: `grep -r "AIza" api/functions/llm-*.R` should return nothing
</verification>

<success_criteria>
1. Migration 006 creates llm_cluster_summary_cache and llm_generation_log tables idempotently
2. llm-cache-repository.R provides hash generation, cache lookup, cache storage, and logging
3. llm-service.R integrates ellmer with structured output types
4. GEMINI_API_KEY is read from environment variable (not hardcoded)
5. All functions have roxygen2 documentation
6. ellmer >= 0.4.0 added to renv.lock
</success_criteria>

<output>
After completion, create `.planning/phases/58-llm-foundation/58-01-SUMMARY.md`
</output>
