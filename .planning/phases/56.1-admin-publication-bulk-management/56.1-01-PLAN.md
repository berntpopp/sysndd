---
phase: 56.1-admin-publication-bulk-management
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - api/endpoints/admin_endpoints.R
  - api/endpoints/publication_endpoints.R
autonomous: true

must_haves:
  truths:
    - "Admin can POST /api/admin/publications/refresh with PMID array and job is created"
    - "Job processes PMIDs sequentially with 350ms rate limiting"
    - "Each PMID is fetched from PubMed and database record is updated"
    - "Progress is reported via existing job progress system"
    - "Admin can GET /api/publications/stats to see total count and oldest update date"
  artifacts:
    - path: "api/endpoints/admin_endpoints.R"
      provides: "POST /admin/publications/refresh endpoint"
      contains: "publication_refresh"
    - path: "api/endpoints/publication_endpoints.R"
      provides: "GET /publications/stats endpoint"
      contains: "publications/stats"
  key_links:
    - from: "admin_endpoints.R"
      to: "publication-functions.R"
      via: "source() and info_from_pmid() call"
      pattern: "source.*publication-functions|info_from_pmid"
    - from: "admin_endpoints.R"
      to: "job-manager.R"
      via: "create_job() call"
      pattern: "create_job"
    - from: "admin_endpoints.R"
      to: "job-progress.R"
      via: "create_progress_reporter() call"
      pattern: "create_progress_reporter"
---

<objective>
Create admin API endpoints for bulk publication metadata refresh from PubMed.

Purpose: Enable administrators to refresh publication metadata from PubMed in bulk, keeping the 4,547 publications current. The existing curation workflow only adds new publications; there's no way to update existing records.

Output: Two new API endpoints - POST /admin/publications/refresh (async job) and GET /publications/stats (stats summary)
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md

# Phase context
@.planning/phases/56.1-admin-publication-bulk-management/56.1-CONTEXT.md
@.planning/phases/56.1-admin-publication-bulk-management/56.1-RESEARCH.md

# Implementation patterns (CRITICAL - follow exactly)
@api/endpoints/admin_endpoints.R (lines 196-337: update_ontology_async pattern)
@api/functions/job-manager.R (lines 65-148: create_job)
@api/functions/job-progress.R (create_progress_reporter)
@api/functions/publication-functions.R (lines 286-333: info_from_pmid)
@api/functions/db-helpers.R (db_execute_statement)
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add publication stats endpoint</name>
  <files>api/endpoints/publication_endpoints.R</files>
  <action>
Add GET /publications/stats endpoint that returns:
- total: count of all publications
- oldest_update: the oldest update_date value (most stale publication)
- outdated_count: count where update_date is older than 1 year

SQL queries:
```sql
SELECT COUNT(*) as total FROM publication
SELECT MIN(update_date) as oldest_update FROM publication
SELECT COUNT(*) as outdated_count FROM publication WHERE update_date < DATE_SUB(NOW(), INTERVAL 1 YEAR)
```

Use pool connection (not async job - quick query).
Add @tag publication, @serializer json list(na="string").
No authentication required (read-only stats).
  </action>
  <verify>
```bash
cd /home/bernt-popp/development/sysndd/api
curl -s "http://localhost:3535/api/publications/stats" | jq .
# Should return: { "total": 4547, "oldest_update": "...", "outdated_count": N }
```
  </verify>
  <done>GET /publications/stats returns total count, oldest update date, and outdated count</done>
</task>

<task type="auto">
  <name>Task 2: Add async publication refresh endpoint</name>
  <files>api/endpoints/admin_endpoints.R</files>
  <action>
Add POST /admin/publications/refresh endpoint following the update_ontology_async pattern (lines 196-337).

Key implementation details:

1. **Extract request body BEFORE mirai** - Request object cannot cross process boundaries:
```r
body <- req$body
pmids <- body$pmids  # Array like ["PMID:12345678", "PMID:87654321"]
```

2. **Validate input**:
```r
if (is.null(pmids) || length(pmids) == 0) {
  res$status <- 400
  return(list(error = "No PMIDs provided"))
}
```

3. **Check for duplicate job**:
```r
dup_check <- check_duplicate_job("publication_refresh", list(pmids = pmids))
```

4. **Create async job with create_job()**:
- operation = "publication_refresh"
- params = list(pmids = pmids, db_config = list(...)) - pass db credentials, NOT pool object
- timeout_ms = 7200000 (2 hours for ~4500 pubs at 350ms each = ~27 min)

5. **executor_fn must**:
- Source required functions: publication-functions.R, job-progress.R, db-helpers.R
- Create progress reporter: `reporter <- create_progress_reporter(params$.__job_id__)`
- Create new database connection (not use pool)
- Process PMIDs sequentially with 350ms delay: `Sys.sleep(0.35)`
- For each PMID:
  - Report progress: `reporter("fetch", sprintf("Fetching %s (%d/%d)", pmid, i, total), i, total)`
  - Call info_from_pmid(pmid) to fetch from PubMed
  - UPDATE publication table with new metadata
  - Catch errors per-PMID (don't fail entire job)

6. **UPDATE statement**:
```r
db_execute_statement(
  "UPDATE publication SET
    Title = ?, Abstract = ?, Publication_date = ?,
    Journal = ?, Keywords = ?, Lastname = ?, Firstname = ?,
    update_date = NOW()
  WHERE publication_id = ?",
  list(info$Title, info$Abstract, info$Publication_date,
       info$Journal, info$Keywords, info$Lastname, info$Firstname,
       pmid),
  conn = sysndd_db
)
```

7. **Return from executor**:
```r
list(
  total = total,
  success = sum(vapply(results, function(r) r$status == "updated", logical(1))),
  failed = sum(vapply(results, function(r) r$status == "error", logical(1))),
  results = results
)
```

8. **Endpoint returns 202 Accepted** with job_id.

Use require_role(req, res, "Administrator") for authorization.

CRITICAL pitfalls from RESEARCH.md:
- DON'T pass pool object to mirai executor (not serializable)
- DON'T forget source() statements in executor_fn
- DON'T forget Sys.sleep(0.35) rate limiting
- DO use explicit conn parameter with db_execute_statement()
  </action>
  <verify>
```bash
cd /home/bernt-popp/development/sysndd/api
# Get admin token
TOKEN=$(curl -s -X POST "http://localhost:3535/api/auth" \
  -H "Content-Type: application/json" \
  -d '{"email":"admin@example.com","password":"adminpass"}' | jq -r '.token')

# Start refresh job with 2 test PMIDs
curl -s -X POST "http://localhost:3535/api/admin/publications/refresh" \
  -H "Authorization: Bearer $TOKEN" \
  -H "Content-Type: application/json" \
  -d '{"pmids":["PMID:12345678","PMID:87654321"]}' | jq .
# Should return: { "job_id": "...", "status": "accepted", "estimated_seconds": 30 }

# Check job status (use returned job_id)
curl -s "http://localhost:3535/api/jobs/{job_id}/status" | jq .
# Should show progress updates
```
  </verify>
  <done>POST /admin/publications/refresh creates async job that processes PMIDs with rate limiting and progress reporting</done>
</task>

<task type="auto">
  <name>Task 3: Add tests for publication refresh endpoint</name>
  <files>api/tests/testthat/test-publication-refresh.R</files>
  <action>
Create unit tests for the publication refresh functionality:

1. **Test stats endpoint** (requires database):
```r
test_that("publications/stats returns expected fields", {
  skip_if(is.null(pool), "Database not available")

  # Call endpoint logic directly
  stats <- get_publication_stats()

  expect_true("total" %in% names(stats))
  expect_true("oldest_update" %in% names(stats))
  expect_true("outdated_count" %in% names(stats))
  expect_type(stats$total, "integer")
})
```

2. **Test PMID validation**:
```r
test_that("publication refresh validates PMID input", {
  # Empty array
  result <- validate_refresh_pmids(c())
  expect_equal(result$error, "No PMIDs provided")

  # Valid array
  result <- validate_refresh_pmids(c("PMID:12345678"))
  expect_null(result$error)
})
```

3. **Test rate limiting logic** (mock test):
```r
test_that("refresh processes PMIDs with delay", {
  # This is a design verification, not integration test
  # Verify Sys.sleep(0.35) is in executor_fn code
  expect_true(TRUE)  # Placeholder - manual code review
})
```

Use skip_if() for database-dependent tests.
Follow existing test patterns in tests/testthat/.
  </action>
  <verify>
```bash
cd /home/bernt-popp/development/sysndd/api
Rscript -e "devtools::test(filter='publication-refresh')"
# All tests should pass
```
  </verify>
  <done>Tests verify stats endpoint fields and PMID validation</done>
</task>

</tasks>

<verification>
1. API lint passes:
```bash
cd /home/bernt-popp/development/sysndd && make lint-api
```

2. Tests pass:
```bash
cd /home/bernt-popp/development/sysndd/api
Rscript -e "devtools::test(filter='publication-refresh')"
```

3. Both endpoints documented in Swagger UI at http://localhost:3535/__docs__/
</verification>

<success_criteria>
- [ ] GET /api/publications/stats returns total, oldest_update, outdated_count
- [ ] POST /api/admin/publications/refresh creates async job with job_id
- [ ] Job processes PMIDs sequentially with 350ms delay (rate limiting)
- [ ] Progress reported via job progress system
- [ ] Each PMID update uses info_from_pmid() and UPDATE statement
- [ ] Errors per-PMID are captured without failing entire job
- [ ] Administrator role required for refresh endpoint
- [ ] Tests pass for stats endpoint and PMID validation
</success_criteria>

<output>
After completion, create `.planning/phases/56.1-admin-publication-bulk-management/56.1-01-SUMMARY.md`
</output>
