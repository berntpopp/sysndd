# Phase 56.1: Admin Publication Bulk Management - Research

**Researched:** 2026-01-31
**Domain:** PubMed API integration + R/Plumber async jobs + Vue 3 admin UI
**Confidence:** HIGH

## Summary

Phase 56.1 adds admin capability to refresh publication metadata from PubMed in bulk. The system already has publication-functions.R with `info_from_pmid()` and `table_articles_from_xml()` that fetch and parse PubMed data for individual publications. This phase extends that pattern to bulk operations via an async job endpoint and ManageAnnotations UI tab.

**Standard approach:**
1. R API: Add admin endpoint using existing `create_job()` pattern from job-manager.R
2. Use easyPubMed library (already in renv.lock) for PubMed E-utilities API
3. Rate limit with 350ms delays (safe for no API key: 3 req/sec limit)
4. Vue UI: Add Publications tab to ManageAnnotations using `useAsyncJob` composable
5. Database: UPDATE publication table using db_execute_statement() with transactions

**Key constraint from CONTEXT.md:** Publications are refreshed in-place (UPDATE existing records), not bulk replaced. The system has 4,547 publications dating to 2025-07-14.

**Primary recommendation:** Follow the HGNC update async job pattern (admin_endpoints.R:340-404) which demonstrates the exact architecture needed: pre-fetch data, create_job with executor_fn, db transaction with UPDATE statements, progress reporting via create_progress_reporter().

## Standard Stack

### Core
| Library | Version | Purpose | Why Standard |
|---------|---------|---------|--------------|
| easyPubMed | 3.2 | PubMed E-utilities API | Already used in publication-functions.R, handles XML parsing |
| mirai | 1.3.1 | Async job execution | Project standard for long-running admin tasks |
| httr2 | 1.0.7 | HTTP client (optional) | Used for external APIs, could replace easyPubMed calls if needed |
| xml2 | 1.3.6 | XML parsing | Used in table_articles_from_xml() for PubMed response parsing |

### Supporting
| Library | Version | Purpose | When to Use |
|---------|---------|---------|-------------|
| promises | 1.3.2 | Promise-based async | Required by mirai for job completion callbacks |
| later | 1.4.1 | Event loop scheduling | Required by mirai for async execution |
| uuid | 1.2-1 | Job ID generation | Already used by create_job() |
| digest | 0.6.37 | Param hashing | Already used for duplicate job detection |

### Frontend
| Library | Version | Purpose | When to Use |
|---------|---------|---------|-------------|
| Bootstrap-Vue-Next | 0.42.0 | UI components | BTable, BProgress, BSpinner for UI |
| VueUse | 11.4.0 | Composables | useIntervalFn for polling (via useAsyncJob) |
| axios | 1.7.9 | HTTP client | API communication |

**Installation:**
Already installed in api/renv.lock and app/package.json. No new dependencies required.

## Architecture Patterns

### Recommended Project Structure
```
api/
├── endpoints/
│   └── admin_endpoints.R        # Add POST /admin/publications/refresh
├── functions/
│   ├── publication-functions.R  # Reuse info_from_pmid(), table_articles_from_xml()
│   ├── job-manager.R           # Reuse create_job(), get_job_status()
│   ├── job-progress.R          # Reuse create_progress_reporter()
│   └── db-helpers.R            # Reuse db_execute_statement()
app/src/
├── views/admin/
│   └── ManageAnnotations.vue   # Add Publications tab
└── composables/
    └── useAsyncJob.ts          # Reuse for progress tracking
```

### Pattern 1: Async Admin Endpoint with PubMed API

**What:** Admin endpoint that creates mirai job for bulk publication refresh
**When to use:** Long-running operations that hit external APIs with rate limits
**Source:** api/endpoints/admin_endpoints.R lines 196-337 (update_ontology_async)

```r
#* Refresh publication metadata from PubMed
#*
#* Returns immediately with job_id. Poll GET /api/jobs/{job_id} for status.
#*
#* @param req Plumber request object
#* @param res Plumber response object
#* @serializer json list(na="string")
#* @tag admin
#* @post /admin/publications/refresh
function(req, res) {
  require_role(req, res, "Administrator")

  # CRITICAL: Extract request body BEFORE mirai
  # (req object cannot cross process boundaries)
  body <- req$body
  pmids <- body$pmids  # Array like ["PMID:12345678", "PMID:87654321"]

  # Validate PMIDs
  if (is.null(pmids) || length(pmids) == 0) {
    res$status <- 400
    return(list(error = "No PMIDs provided"))
  }

  # Check for duplicate job
  dup_check <- check_duplicate_job("publication_refresh", list(pmids = pmids))
  if (dup_check$duplicate) {
    return(list(
      job_id = dup_check$existing_job_id,
      status = "already_running"
    ))
  }

  # Create async job
  result <- create_job(
    operation = "publication_refresh",
    params = list(
      pmids = pmids,
      db_config = list(
        dbname = dw$dbname,
        user = dw$user,
        password = dw$password,
        host = dw$host,
        port = dw$port
      )
    ),
    executor_fn = function(params) {
      # This runs in mirai daemon - must source required functions
      source("functions/publication-functions.R")
      source("functions/job-progress.R")
      source("functions/db-helpers.R")

      # Create progress reporter
      reporter <- create_progress_reporter(params$.__job_id__)

      pmids <- params$pmids
      total <- length(pmids)
      results <- list()

      # Process each PMID with rate limiting
      for (i in seq_along(pmids)) {
        pmid <- pmids[i]
        reporter("fetch", sprintf("Fetching %s (%d/%d)", pmid, i, total), i, total)

        result <- tryCatch({
          # Fetch from PubMed
          info <- info_from_pmid(pmid)

          # Update database
          db_execute_statement(
            "UPDATE publication SET
              Title = ?, Abstract = ?, Publication_date = ?,
              Journal = ?, Keywords = ?, Lastname = ?, Firstname = ?,
              update_date = NOW()
            WHERE publication_id = ?",
            list(
              info$Title, info$Abstract, info$Publication_date,
              info$Journal, info$Keywords, info$Lastname, info$Firstname,
              pmid
            )
          )

          list(pmid = pmid, status = "updated")
        }, error = function(e) {
          list(pmid = pmid, status = "error", error = e$message)
        })

        results[[i]] <- result

        # Rate limit: 350ms delay (safe for 3 req/sec without API key)
        if (i < total) Sys.sleep(0.35)
      }

      list(
        total = total,
        results = results,
        success = sum(vapply(results, function(r) r$status == "updated", logical(1))),
        failed = sum(vapply(results, function(r) r$status == "error", logical(1)))
      )
    },
    timeout_ms = 7200000  # 2 hours for large batches
  )

  res$status <- 202
  return(result)
}
```

### Pattern 2: Reuse Existing PubMed Functions

**What:** info_from_pmid() already handles PubMed API calls and XML parsing
**When to use:** Refreshing publication metadata (no new publications, just updates)
**Source:** api/functions/publication-functions.R lines 286-333

**Key insight:** The existing function does everything needed:
- Validates PMID format (strips "PMID:" prefix)
- Calls easyPubMed get_pubmed_ids() and fetch_pubmed_data()
- Parses XML with table_articles_from_xml()
- Returns tibble with Title, Abstract, Publication_date, Journal, Keywords, etc.

**For bulk refresh:**
```r
# DON'T chunk PMIDs into batches of 200 like new_publication() does
# For refresh, process one at a time with rate limiting for safety
for (pmid in pmids) {
  info <- info_from_pmid(pmid)  # Single PMID, returns 1-row tibble
  # UPDATE database with info$Title, info$Abstract, etc.
  Sys.sleep(0.35)  # Rate limit
}
```

### Pattern 3: ManageAnnotations Tab Pattern

**What:** Add Publications tab following Deprecated OMIM Entities pattern
**When to use:** Admin tables with stats, actions, and optional data display
**Source:** app/src/views/admin/ManageAnnotations.vue lines 231-387

```vue
<!-- Add to ManageAnnotations.vue -->
<BRow class="justify-content-md-center py-2">
  <BCol col md="12">
    <BCard
      header-tag="header"
      body-class="p-2"
      header-class="p-1"
      border-variant="dark"
      class="mb-3 text-start"
    >
      <template #header>
        <h5 class="mb-0 text-start font-weight-bold">
          Publication Metadata Refresh
          <span class="badge bg-info ms-2 fw-normal">
            {{ publicationStats.total?.toLocaleString() }} publications
          </span>
          <span v-if="publicationStats.oldest_update" class="badge bg-warning text-dark ms-2">
            Oldest: {{ formatDate(publicationStats.oldest_update) }}
          </span>
        </h5>
      </template>

      <!-- Stats row -->
      <div class="mb-3">
        <p class="text-muted small mb-2">
          Refresh publication metadata from PubMed. Publications are updated in place
          (no deletions). Rate limited to 3 requests/second.
        </p>
      </div>

      <!-- Action buttons -->
      <div class="d-flex gap-2 mb-3">
        <BButton
          variant="primary"
          :disabled="refreshJob.isLoading.value"
          @click="refreshAllPublications"
        >
          <BSpinner v-if="refreshJob.isLoading.value" small type="grow" class="me-2" />
          {{ refreshJob.isLoading.value ? 'Refreshing...' : 'Refresh All Publications' }}
        </BButton>
      </div>

      <!-- Progress display -->
      <div v-if="refreshJob.isLoading.value || refreshJob.status.value !== 'idle'" class="mt-3">
        <div class="d-flex align-items-center mb-2">
          <span class="badge me-2" :class="refreshJob.statusBadgeClass.value">
            {{ refreshJob.status.value }}
          </span>
          <span class="text-muted">{{ refreshJob.step.value }}</span>
        </div>

        <BProgress
          v-if="refreshJob.isLoading.value"
          :value="refreshJob.hasRealProgress.value ? refreshJob.progressPercent.value : 100"
          :max="100"
          :animated="true"
          :striped="!refreshJob.hasRealProgress.value"
          :variant="refreshJob.progressVariant.value"
          height="1.5rem"
        >
          <template #default>
            <span v-if="refreshJob.hasRealProgress.value">
              {{ refreshJob.progressPercent.value }}% - {{ refreshJob.step.value }}
            </span>
            <span v-else>
              {{ refreshJob.step.value }} ({{ refreshJob.elapsedTimeDisplay.value }})
            </span>
          </template>
        </BProgress>

        <div v-if="refreshJob.progress.value.current && refreshJob.progress.value.total"
             class="small text-muted mt-1">
          {{ refreshJob.progress.value.current.toLocaleString() }} /
          {{ refreshJob.progress.value.total.toLocaleString() }}
          ({{ refreshJob.elapsedTimeDisplay.value }})
        </div>
      </div>
    </BCard>
  </BCol>
</BRow>
```

**TypeScript composable setup:**
```typescript
// In <script setup>
const refreshJob = useAsyncJob(
  (jobId: string) => `${import.meta.env.VITE_API_URL}/api/jobs/${jobId}/status`
);

async function refreshAllPublications() {
  refreshJob.reset();

  try {
    // Get all PMIDs from publications
    const response = await axios.get(
      `${import.meta.env.VITE_API_URL}/api/publications`,
      { headers: { Authorization: `Bearer ${localStorage.getItem('token')}` } }
    );

    const pmids = response.data.map(p => p.publication_id);

    // Start refresh job
    const jobResponse = await axios.post(
      `${import.meta.env.VITE_API_URL}/api/admin/publications/refresh`,
      { pmids },
      { headers: { Authorization: `Bearer ${localStorage.getItem('token')}` } }
    );

    refreshJob.startJob(jobResponse.data.job_id);
  } catch (error) {
    makeToast('Failed to start publication refresh', 'Error', 'danger');
  }
}

// Watch for completion
watch(
  () => refreshJob.status.value,
  (newStatus) => {
    if (newStatus === 'completed') {
      makeToast('Publications refreshed successfully', 'Success', 'success');
      fetchPublicationStats();  // Refresh stats
    } else if (newStatus === 'failed') {
      makeToast(refreshJob.error.value || 'Refresh failed', 'Error', 'danger');
    }
  }
);
```

### Anti-Patterns to Avoid

- **Synchronous endpoint:** Don't use blocking PUT endpoint like old update_ontology. Use create_job() for async execution.
- **Batch PubMed requests:** Don't fetch 200 PMIDs at once like new_publication() does. Rate limits are strict without API key.
- **Client-side filtering:** Don't fetch all publications to frontend then POST selected PMIDs. Use API-only filtering (project preference from CONTEXT.md).
- **Missing rate limiting:** easyPubMed doesn't auto-throttle. Must add Sys.sleep(0.35) between requests.
- **Direct pool usage in mirai:** Don't pass pool object to executor_fn. Pass db_config, create new connection in daemon.

## Don't Hand-Roll

| Problem | Don't Build | Use Instead | Why |
|---------|-------------|-------------|-----|
| Job state management | Custom job tracking in database | create_job() from job-manager.R | Handles mirai objects, promise callbacks, capacity limits, duplicate detection |
| Progress tracking | Poll database for completion | create_progress_reporter() from job-progress.R | File-based progress, atomic writes, automatic cleanup |
| Frontend job polling | setInterval loop | useAsyncJob composable | Auto-cleanup on unmount, handles R array unwrapping, status badges |
| Rate limiting | Manual timestamp tracking | Sys.sleep() with fixed delay | PubMed rate limits are simple (3 req/sec), no need for token bucket |
| XML parsing | Custom PubMed XML parser | table_articles_from_xml() | Already handles edge cases (collective names, missing dates, multiple DOI locations) |
| Database transactions | Manual BEGIN/COMMIT | db_with_transaction() from db-helpers.R | Auto-rollback on error, connection cleanup |

**Key insight:** The codebase already has every building block needed. This phase is 90% composition, 10% new code.

## Common Pitfalls

### Pitfall 1: Forgetting Rate Limits
**What goes wrong:** Hitting PubMed API without delays triggers 429 errors or IP blocks
**Why it happens:** easyPubMed library has no built-in throttling
**How to avoid:** Add Sys.sleep(0.35) between every request (safe for 3 req/sec limit without API key)
**Warning signs:** HTTP 429 responses, inconsistent fetch failures

**Example from CONTEXT.md:**
```r
# NCBI rate limits:
# - Without API key: 3 requests/second
# - With NCBI_API_KEY: 10 requests/second
# Recommendation: 350ms delay = 2.86 req/sec (safe margin)
for (i in seq_along(pmids)) {
  fetch_publication(pmids[i])
  if (i < length(pmids)) Sys.sleep(0.35)  # CRITICAL
}
```

### Pitfall 2: Passing Pool Object to Mirai Daemon
**What goes wrong:** Database connections cannot cross process boundaries, executor crashes
**Why it happens:** mirai runs in separate daemon process, pool object is not serializable
**How to avoid:** Pass db_config list, create new DBI connection in executor_fn
**Warning signs:** "cannot open the connection" errors, serialization failures

**Example from admin_endpoints.R:303-312:**
```r
# CORRECT: Pass config, create connection in daemon
result <- create_job(
  params = list(
    db_config = list(
      dbname = dw$dbname,
      user = dw$user,
      password = dw$password,
      host = dw$host,
      port = dw$port
    )
  ),
  executor_fn = function(params) {
    sysndd_db <- DBI::dbConnect(
      RMariaDB::MariaDB(),
      dbname = params$db_config$dbname,
      user = params$db_config$user,
      password = params$db_config$password,
      host = params$db_config$host,
      port = params$db_config$port
    )
    on.exit(DBI::dbDisconnect(sysndd_db), add = TRUE)

    # Use db_execute_statement() with explicit conn parameter
    db_execute_statement(sql, params, conn = sysndd_db)
  }
)

# WRONG: Passing pool object
result <- create_job(
  params = list(pool = pool),  # DON'T DO THIS
  executor_fn = function(params) {
    pool %>% tbl("publication")  # FAILS: pool not serializable
  }
)
```

### Pitfall 3: Missing Source Statements in Executor
**What goes wrong:** executor_fn cannot find functions like info_from_pmid(), crashes with "object not found"
**Why it happens:** Mirai daemon starts fresh, doesn't inherit global environment
**How to avoid:** Explicitly source() all required function files at start of executor_fn
**Warning signs:** "could not find function" errors in job results

**Example from admin_endpoints.R:272-275:**
```r
executor_fn = function(params) {
  # Source required functions in worker (MANDATORY)
  source("functions/omim-functions.R")
  source("functions/mondo-functions.R")
  source("functions/ontology-functions.R")
  source("functions/file-functions.R")

  # Now functions are available
  disease_ontology_set_update <- process_combine_ontology(...)
}
```

### Pitfall 4: Not Handling easyPubMed API Changes
**What goes wrong:** extract_pubmed_count() breaks when easyPubMed switches from list to S4 object
**Why it happens:** easyPubMed has changed API structure between versions
**How to avoid:** Use defensive extraction with tryCatch() for both old and new formats
**Warning signs:** "$ operator is invalid for atomic vectors" or slot access errors

**Example from publication-functions.R:25-48:**
```r
extract_pubmed_count <- function(pubmed_result) {
  # Try S4 slot access first (newer easyPubMed versions)
  tryCatch(
    {
      if (isS4(pubmed_result)) {
        return(pubmed_result@count)
      } else {
        # Fall back to list access (older versions)
        return(pubmed_result$Count)
      }
    },
    error = function(e) {
      # If both fail, try alternative approaches
      tryCatch(
        {
          return(pubmed_result$Count)
        },
        error = function(e2) {
          return(0)
        }
      )
    }
  )
}
```

### Pitfall 5: Not Unwrapping R/Plumber Arrays in Vue
**What goes wrong:** Frontend shows "[object Object]" or array values instead of scalars
**Why it happens:** R/Plumber serializes scalars as single-element arrays in JSON
**How to avoid:** Use unwrapValue() helper in useAsyncJob composable
**Warning signs:** Seeing [123] instead of 123, ["running"] instead of "running"

**Example from useAsyncJob.ts:199-201:**
```typescript
function unwrapValue<T>(val: T | T[]): T {
  return Array.isArray(val) && val.length === 1 ? val[0] : (val as T);
}

// Use when reading API responses
status.value = unwrapValue(data.status) as JobStatus;
```

## Code Examples

### Example 1: Async Publication Refresh Endpoint

Verified pattern from admin_endpoints.R adapted for publication refresh:

```r
# api/endpoints/admin_endpoints.R

#* Refresh publication metadata from PubMed
#*
#* Returns immediately with job_id. Poll GET /api/jobs/{job_id} for status.
#* Rate limited to 3 requests/second (NCBI limit without API key).
#*
#* # `Request Body`
#* {
#*   "pmids": ["PMID:12345678", "PMID:87654321"]
#* }
#*
#* # `Response (202 Accepted)`
#* {
#*   "job_id": "550e8400-e29b-41d4-a716-446655440000",
#*   "status": "accepted",
#*   "estimated_seconds": 30
#* }
#*
#* # `Authorization`
#* Restricted to Administrator role.
#*
#* @tag admin
#* @serializer json list(na="string")
#* @post /admin/publications/refresh
function(req, res) {
  require_role(req, res, "Administrator")

  # CRITICAL: Extract request body BEFORE mirai call
  # Request object cannot cross process boundaries
  body <- req$body
  pmids <- body$pmids

  # Validate input
  if (is.null(pmids) || length(pmids) == 0) {
    res$status <- 400
    return(list(error = "No PMIDs provided"))
  }

  # Check for duplicate job
  dup_check <- check_duplicate_job("publication_refresh", list(pmids = pmids))
  if (dup_check$duplicate) {
    return(list(
      job_id = dup_check$existing_job_id,
      status = "already_running",
      message = "A publication refresh job is already running with these PMIDs"
    ))
  }

  # Create async job
  result <- create_job(
    operation = "publication_refresh",
    params = list(
      pmids = pmids,
      db_config = list(
        dbname = dw$dbname,
        user = dw$user,
        password = dw$password,
        server = dw$server,
        host = dw$host,
        port = dw$port
      )
    ),
    executor_fn = function(params) {
      # Source required functions in mirai daemon
      source("functions/publication-functions.R")
      source("functions/job-progress.R")
      source("functions/db-helpers.R")

      # Create progress reporter
      reporter <- create_progress_reporter(params$.__job_id__)

      pmids <- params$pmids
      total <- length(pmids)
      results <- list()

      # Connect to database (daemon needs its own connection)
      sysndd_db <- DBI::dbConnect(
        RMariaDB::MariaDB(),
        dbname = params$db_config$dbname,
        user = params$db_config$user,
        password = params$db_config$password,
        server = params$db_config$server,
        host = params$db_config$host,
        port = params$db_config$port
      )
      on.exit(DBI::dbDisconnect(sysndd_db), add = TRUE)

      # Process each PMID with rate limiting
      for (i in seq_along(pmids)) {
        pmid <- pmids[i]
        reporter(
          "fetch",
          sprintf("Fetching %s (%d/%d)", pmid, i, total),
          current = i,
          total = total
        )

        result_item <- tryCatch({
          # Fetch metadata from PubMed
          info <- info_from_pmid(pmid)

          # Update database
          rows_affected <- db_execute_statement(
            "UPDATE publication SET
              Title = ?,
              Abstract = ?,
              Publication_date = ?,
              Journal = ?,
              Keywords = ?,
              Lastname = ?,
              Firstname = ?,
              update_date = NOW()
            WHERE publication_id = ?",
            list(
              info$Title,
              info$Abstract,
              info$Publication_date,
              info$Journal,
              info$Keywords,
              info$Lastname,
              info$Firstname,
              pmid
            ),
            conn = sysndd_db
          )

          list(
            pmid = pmid,
            status = if (rows_affected > 0) "updated" else "not_found",
            title = info$Title
          )
        }, error = function(e) {
          list(
            pmid = pmid,
            status = "error",
            error = e$message
          )
        })

        results[[i]] <- result_item

        # Rate limit: 350ms delay = 2.86 req/sec (safe for NCBI 3 req/sec limit)
        if (i < total) Sys.sleep(0.35)
      }

      # Return summary
      list(
        total = total,
        success = sum(vapply(results, function(r) r$status == "updated", logical(1))),
        failed = sum(vapply(results, function(r) r$status == "error", logical(1))),
        not_found = sum(vapply(results, function(r) r$status == "not_found", logical(1))),
        results = results
      )
    },
    timeout_ms = 7200000  # 2 hours timeout for large batches (4547 publications ~27 min)
  )

  res$status <- 202
  return(result)
}
```

### Example 2: Frontend Publication Management Component

Using useAsyncJob composable pattern from ManageAnnotations.vue:

```typescript
// app/src/views/admin/ManageAnnotations.vue
// Add to <script setup>

import { useAsyncJob } from '@/composables/useAsyncJob';

// Create job instance for publication refresh
const refreshJob = useAsyncJob(
  (jobId: string) => `${import.meta.env.VITE_API_URL}/api/jobs/${jobId}/status`
);

// Publication stats state
const publicationStats = ref({
  total: null as number | null,
  oldest_update: null as string | null,
  outdated_count: null as number | null,
});

// Fetch publication statistics
async function fetchPublicationStats() {
  try {
    const response = await axios.get(
      `${import.meta.env.VITE_API_URL}/api/publications/stats`,
      {
        headers: {
          Authorization: `Bearer ${localStorage.getItem('token')}`,
        },
      }
    );

    publicationStats.value = {
      total: unwrapValue(response.data.total),
      oldest_update: unwrapValue(response.data.oldest_update),
      outdated_count: unwrapValue(response.data.outdated_count),
    };
  } catch (error) {
    console.warn('Failed to fetch publication stats:', error);
  }
}

// Refresh all publications
async function refreshAllPublications() {
  refreshJob.reset();

  try {
    // Get all publication PMIDs
    const response = await axios.get(
      `${import.meta.env.VITE_API_URL}/api/publications`,
      {
        headers: {
          Authorization: `Bearer ${localStorage.getItem('token')}`,
        },
        params: {
          fields: 'publication_id',
        },
      }
    );

    const pmids = response.data.map((p: any) => unwrapValue(p.publication_id));

    if (pmids.length === 0) {
      makeToast('No publications to refresh', 'Info', 'info');
      return;
    }

    // Start refresh job
    const jobResponse = await axios.post(
      `${import.meta.env.VITE_API_URL}/api/admin/publications/refresh`,
      { pmids },
      {
        headers: {
          Authorization: `Bearer ${localStorage.getItem('token')}`,
        },
      }
    );

    if (jobResponse.data.error) {
      makeToast(jobResponse.data.message || 'Failed to start refresh', 'Error', 'danger');
      return;
    }

    // Start tracking the job
    refreshJob.startJob(jobResponse.data.job_id);
  } catch (error) {
    makeToast('Failed to start publication refresh', 'Error', 'danger');
    console.error('Refresh error:', error);
  }
}

// Watch for job completion
watch(
  () => refreshJob.status.value,
  (newStatus) => {
    if (newStatus === 'completed') {
      makeToast('Publications refreshed successfully', 'Success', 'success');
      fetchPublicationStats();  // Refresh stats
      fetchJobHistory();        // Update job history
    } else if (newStatus === 'failed') {
      const errorMsg = refreshJob.error.value || 'Publication refresh failed';
      makeToast(errorMsg, 'Error', 'danger');
      fetchJobHistory();
    }
  }
);

// Load stats on mount
onMounted(() => {
  fetchPublicationStats();
});
```

## State of the Art

| Old Approach | Current Approach | When Changed | Impact |
|--------------|------------------|--------------|--------|
| Manual publication updates only | Add bulk admin refresh | Phase 56.1 (2026-01) | Admins can keep 4,547 publications current |
| Synchronous admin endpoints | Async jobs with mirai | Phase 56 (2025) | Long-running operations don't block API |
| easyPubMed list API | easyPubMed S4 objects | easyPubMed 3.x | Must use extract_pubmed_count() helper |
| Client-side progress polling | Server-side progress files | job-progress.R (2025) | Real-time progress without database polling |

**Deprecated/outdated:**
- Synchronous PUT /admin/update_ontology endpoint (still exists but marked DEPRECATED in favor of async version)
- Direct pool usage in mirai executors (now pass db_config and create connection in daemon)

## Open Questions

1. **Should we add selective refresh (by date or PMID list)?**
   - What we know: CONTEXT.md proposes "refresh outdated" endpoint (older_than_days parameter)
   - What's unclear: User workflow - do admins need to see which publications are stale before refreshing?
   - Recommendation: Start with "refresh all" button. If needed, add filter UI in later iteration.

2. **Do we need an endpoint to list publications for the UI?**
   - What we know: refreshAllPublications() needs to fetch all PMIDs to POST to refresh endpoint
   - What's unclear: Does an endpoint like GET /api/publications exist? (not checked during research)
   - Recommendation: Check for existing endpoint. If missing, add GET /api/publications with fields parameter.

3. **Should progress show publication titles or just PMIDs?**
   - What we know: Progress reporter can include current PMID in message
   - What's unclear: Fetching title requires parsing response, might slow down progress updates
   - Recommendation: Show PMID only in progress (fast). Show titles in results array after completion.

## Sources

### Primary (HIGH confidence)
- api/functions/publication-functions.R - Existing PubMed integration (lines 286-333: info_from_pmid)
- api/functions/job-manager.R - Async job infrastructure (lines 65-148: create_job)
- api/functions/job-progress.R - Progress reporting (lines 35-79: create_progress_reporter)
- api/endpoints/admin_endpoints.R - Admin async patterns (lines 196-337: update_ontology_async)
- app/src/views/admin/ManageAnnotations.vue - UI patterns (lines 231-387: Deprecated OMIM section)
- app/src/composables/useAsyncJob.ts - Frontend job tracking (complete file)
- api/renv.lock - Verified easyPubMed 3.2 in dependencies (grep result)
- .planning/phases/56.1-admin-publication-bulk-management/56.1-CONTEXT.md - User decisions and constraints

### Secondary (MEDIUM confidence)
- NCBI E-utilities documentation - Rate limits verified via CONTEXT.md references
- easyPubMed package documentation - API structure from extract_pubmed_count() implementation

### Tertiary (LOW confidence)
- None - all findings verified with codebase inspection

## Metadata

**Confidence breakdown:**
- Standard stack: HIGH - All libraries already in renv.lock/package.json, verified by reading files
- Architecture: HIGH - Patterns extracted from working admin_endpoints.R (update_ontology_async, update_hgnc_data)
- Pitfalls: HIGH - Documented in code comments and handled by existing helper functions

**Research date:** 2026-01-31
**Valid until:** 2026-03-02 (30 days - stable APIs, existing codebase patterns)

**Key constraint honored:** CONTEXT.md specifies API-only filtering preferred (not client-side). Publication tab will use backend endpoint to get PMID list rather than loading all publication data to frontend.
