---
phase: 59-llm-batch-caching
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - api/functions/llm-batch-generator.R
  - api/functions/job-manager.R
  - api/endpoints/jobs_endpoints.R
  - api/tests/testthat/test-llm-batch.R
autonomous: true

must_haves:
  truths:
    - "Clustering job completion triggers LLM summary generation automatically"
    - "LLM generation processes each cluster with cache-first lookup"
    - "Per-cluster progress visible during batch generation"
    - "Failed clusters do not stop the entire batch"
    - "Cached clusters are skipped (not regenerated)"
  artifacts:
    - path: "api/functions/llm-batch-generator.R"
      provides: "Batch orchestration with trigger_llm_batch_generation and llm_batch_executor"
      exports: ["trigger_llm_batch_generation", "llm_batch_executor"]
    - path: "api/functions/job-manager.R"
      provides: "LLM job chaining from clustering completion callback"
      contains: "trigger_llm_batch_generation"
    - path: "api/endpoints/jobs_endpoints.R"
      provides: "llm_generation operation message in get_progress_message"
      contains: "llm_generation"
    - path: "api/tests/testthat/test-llm-batch.R"
      provides: "Unit tests for batch generator functions"
      min_lines: 50
  key_links:
    - from: "api/functions/job-manager.R"
      to: "api/functions/llm-batch-generator.R"
      via: "trigger_llm_batch_generation call in promise callback"
      pattern: "trigger_llm_batch_generation"
    - from: "api/functions/llm-batch-generator.R"
      to: "api/functions/llm-service.R"
      via: "generate_cluster_summary call"
      pattern: "generate_cluster_summary"
    - from: "api/functions/llm-batch-generator.R"
      to: "api/functions/llm-cache-repository.R"
      via: "get_cached_summary and save_summary_to_cache calls"
      pattern: "get_cached_summary|save_summary_to_cache"
---

<objective>
Create batch LLM generation orchestrator that chains after clustering jobs, processes clusters with cache-first lookup, and reports per-cluster progress.

Purpose: Implement LLM-05 (batch generation chained after clustering) and LLM-06 (hash-based cache invalidation) from requirements.

Output:
- api/functions/llm-batch-generator.R (batch orchestration)
- Modified job-manager.R (job chaining)
- Unit tests for batch generator
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/59-llm-batch-caching/59-CONTEXT.md
@.planning/phases/59-llm-batch-caching/59-RESEARCH.md
@.planning/phases/58-llm-foundation/58-01-SUMMARY.md
@.planning/phases/58-llm-foundation/58-02-SUMMARY.md
@api/functions/llm-service.R
@api/functions/llm-cache-repository.R
@api/functions/llm-validation.R
@api/functions/job-manager.R
@api/functions/job-progress.R
@api/endpoints/jobs_endpoints.R
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create LLM batch generator module</name>
  <files>api/functions/llm-batch-generator.R</files>
  <action>
Create new file `api/functions/llm-batch-generator.R` with:

1. **Module header and dependencies:**
   - Header comment explaining purpose
   - require(logger), require(ellmer)
   - Source llm-service.R, llm-cache-repository.R, job-progress.R if not loaded

2. **trigger_llm_batch_generation(clusters, cluster_type, parent_job_id):**
   - Check if Gemini is configured via is_gemini_configured()
   - If not configured, log warning and return list(skipped = TRUE, reason = "...")
   - Call create_job() with:
     - operation = "llm_generation"
     - params = list(clusters, cluster_type, parent_job_id)
     - executor_fn = llm_batch_executor
     - timeout_ms = 3600000 (1 hour for large batches)
   - Return job creation result

3. **llm_batch_executor(params):**
   - Extract clusters, cluster_type, job_id from params
   - Create progress reporter via create_progress_reporter(job_id)
   - Initialize counters: total, succeeded, failed, skipped
   - Loop through each cluster:
     a. Update progress: "Cluster N (X/Y)"
     b. Build cluster_data from cluster row (identifiers, term_enrichment, cluster_number)
     c. Generate cluster_hash via generate_cluster_hash()
     d. Check cache via get_cached_summary() - if hit, increment skipped and continue
     e. Attempt generation with retry (max 3 attempts):
        - Call generate_cluster_summary() from llm-service.R
        - On success: save to cache via save_summary_to_cache(), increment succeeded
        - On failure: exponential backoff with jitter, retry
        - After 3 failures: log warning, increment failed, continue to next cluster
   - Final progress update: "Done: X succeeded, Y failed, Z cached"
   - Return summary list(total, succeeded, failed, skipped)

4. **Key implementation details:**
   - Use 2^attempts + runif(1) for backoff timing
   - Do NOT stop batch on individual cluster failures
   - Log each cluster's outcome (success/fail/skip)
   - Handle both functional and phenotype cluster types
  </action>
  <verify>
Run: `grep -E "trigger_llm_batch_generation|llm_batch_executor" api/functions/llm-batch-generator.R`
Expect: Both function names present
Run: `head -50 api/functions/llm-batch-generator.R`
Expect: Module header and require statements visible
  </verify>
  <done>
llm-batch-generator.R exists with trigger_llm_batch_generation() and llm_batch_executor() functions that handle batch generation with cache-first lookup, retry logic, and progress reporting.
  </done>
</task>

<task type="auto">
  <name>Task 2: Add job chaining to job-manager.R</name>
  <files>api/functions/job-manager.R, api/endpoints/jobs_endpoints.R</files>
  <action>
**Modify api/functions/job-manager.R:**

1. At the top of the file (after existing sources), add conditional source for llm-batch-generator.R:
   ```r
   # Load LLM batch generator if available
   if (file.exists("functions/llm-batch-generator.R")) {
     source("functions/llm-batch-generator.R", local = FALSE)
   }
   ```

2. In the promise callback of create_job() (the `m %...>%` section around line 119-131), after the successful completion block (`jobs_env[[job_id]]$status <- "completed"`), add job chaining logic:
   ```r
   # Chain LLM generation after clustering jobs
   if (jobs_env[[job_id]]$operation %in% c("clustering", "phenotype_clustering")) {
     # Determine cluster type from operation
     chain_cluster_type <- if (jobs_env[[job_id]]$operation == "clustering") {
       "functional"
     } else {
       "phenotype"
     }

     # Extract clusters from result (different structure for each type)
     chain_clusters <- if (!is.null(result$clusters)) {
       result$clusters
     } else if (is.data.frame(result)) {
       result
     } else {
       NULL
     }

     if (!is.null(chain_clusters) && nrow(chain_clusters) > 0) {
       # Check if LLM batch generator is available
       if (exists("trigger_llm_batch_generation", mode = "function")) {
         trigger_llm_batch_generation(
           clusters = chain_clusters,
           cluster_type = chain_cluster_type,
           parent_job_id = job_id
         )
       }
     }
   }
   ```

**Modify api/endpoints/jobs_endpoints.R:**

3. Find the get_progress_message() function (around line 261-274) and add llm_generation to the messages list:
   ```r
   llm_generation = "Generating LLM summaries for clusters..."
   ```

**Important:** The chaining happens in the promise callback (main process), NOT in the mirai daemon. This is correct because:
- Clustering job completes in daemon, returns result
- Promise callback fires in main process with result
- Chaining logic can safely call create_job() (which creates a new mirai)
  </action>
  <verify>
Run: `grep -A5 "Chain LLM generation" api/functions/job-manager.R`
Expect: Chaining logic visible with trigger_llm_batch_generation call
Run: `grep "llm_generation" api/endpoints/jobs_endpoints.R`
Expect: Progress message for llm_generation operation
  </verify>
  <done>
Job chaining added to job-manager.R promise callback. Clustering and phenotype_clustering jobs now trigger LLM generation on completion. Progress message added for llm_generation operation.
  </done>
</task>

<task type="auto">
  <name>Task 3: Create unit tests for batch generator</name>
  <files>api/tests/testthat/test-llm-batch.R</files>
  <action>
Create new file `api/tests/testthat/test-llm-batch.R` with:

1. **Test setup:**
   ```r
   # Test file: test-llm-batch.R
   # Unit tests for LLM batch generator functions

   # Skip if database not available (integration tests)
   skip_if_no_db <- function() {
     skip_if(!exists("db_execute_query", mode = "function"), "Database helpers not loaded")
   }
   ```

2. **Tests for trigger_llm_batch_generation():**
   - Test: returns skipped=TRUE when GEMINI_API_KEY not set
     - Temporarily unset GEMINI_API_KEY, call function, verify result
   - Test: accepts valid cluster tibble structure
     - Create mock cluster tibble with required columns
     - Verify function doesn't error (actual job creation requires mirai)

3. **Tests for llm_batch_executor() logic (mocked):**
   - Test: skips cached clusters correctly
     - Mock get_cached_summary to return non-empty result
     - Verify skipped count increments
   - Test: handles empty cluster list
     - Pass empty tibble, verify returns total=0
   - Test: returns correct summary structure
     - Verify result has total, succeeded, failed, skipped keys

4. **Test patterns:**
   - Use `skip_if_no_db()` for tests requiring database
   - Use `skip_if(!is_gemini_configured())` for tests requiring API
   - Mock functions where possible to test logic without side effects
   - Test edge cases: empty input, NULL input, malformed clusters

5. **Note:** Full integration tests require mirai daemons and database - these are unit tests for logic validation.
  </action>
  <verify>
Run: `wc -l api/tests/testthat/test-llm-batch.R`
Expect: At least 50 lines
Run: `grep -c "test_that" api/tests/testthat/test-llm-batch.R`
Expect: At least 4 test cases
  </verify>
  <done>
Unit tests created in test-llm-batch.R covering trigger function behavior, executor logic, edge cases, and error handling. Tests properly skip when database/API not available.
  </done>
</task>

</tasks>

<verification>
1. llm-batch-generator.R exists and exports required functions
2. job-manager.R chains LLM generation after clustering completion
3. jobs_endpoints.R has llm_generation progress message
4. Unit tests exist with at least 4 test cases
5. R files load without syntax errors:
   ```bash
   cd api && Rscript -e "source('functions/llm-batch-generator.R')" 2>&1 | head -5
   ```
</verification>

<success_criteria>
1. trigger_llm_batch_generation() creates an LLM generation job when called
2. llm_batch_executor() processes clusters with cache-first, retry, and progress reporting
3. Clustering job completion automatically triggers LLM generation via promise callback
4. Per-cluster progress format: "Cluster N (X/Y)" visible in job status
5. Failed clusters logged but don't stop batch
6. Unit tests cover key scenarios and edge cases
</success_criteria>

<output>
After completion, create `.planning/phases/59-llm-batch-caching/59-01-SUMMARY.md`
</output>
