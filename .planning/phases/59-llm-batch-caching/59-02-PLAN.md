---
phase: 59-llm-batch-caching
plan: 02
type: execute
wave: 2
depends_on: ["59-01"]
files_modified:
  - api/functions/llm-judge.R
  - api/functions/llm-batch-generator.R
  - api/functions/llm-cache-repository.R
  - api/tests/testthat/test-llm-judge.R
autonomous: true

must_haves:
  truths:
    - "LLM-as-judge validates each summary before final caching"
    - "Validation uses accept/low_confidence/reject verdicts"
    - "Accepted summaries cached with validation_status = validated"
    - "Low_confidence summaries cached with validation_status = pending (flagged)"
    - "Rejected summaries trigger regeneration (up to 3 attempts)"
    - "Judge verdict and reasoning stored in summary metadata"
  artifacts:
    - path: "api/functions/llm-judge.R"
      provides: "LLM-as-judge validation with validate_with_llm_judge"
      exports: ["llm_judge_verdict_type", "validate_with_llm_judge", "generate_and_validate_with_judge"]
    - path: "api/functions/llm-batch-generator.R"
      provides: "Integration of LLM-as-judge in batch executor"
      contains: "generate_and_validate_with_judge"
    - path: "api/functions/llm-cache-repository.R"
      provides: "Update validation status function"
      exports: ["update_validation_status"]
    - path: "api/tests/testthat/test-llm-judge.R"
      provides: "Unit tests for LLM-as-judge functions"
      min_lines: 50
  key_links:
    - from: "api/functions/llm-batch-generator.R"
      to: "api/functions/llm-judge.R"
      via: "generate_and_validate_with_judge call"
      pattern: "generate_and_validate_with_judge"
    - from: "api/functions/llm-judge.R"
      to: "api/functions/llm-service.R"
      via: "generate_cluster_summary call"
      pattern: "generate_cluster_summary"
    - from: "api/functions/llm-judge.R"
      to: "api/functions/llm-cache-repository.R"
      via: "save_summary_to_cache with validation_status"
      pattern: "save_summary_to_cache.*validation_status"
---

<objective>
Implement LLM-as-judge validation as the final pipeline step, with three-tier verdict system (accept/low_confidence/reject) and proper cache status tracking.

Purpose: Implement LLM-09 (LLM-as-judge validates accuracy) and LLM-10 (confidence scoring with metadata) from requirements.

Output:
- api/functions/llm-judge.R (judge validation module)
- Modified llm-batch-generator.R (integrate judge into pipeline)
- Modified llm-cache-repository.R (update_validation_status function)
- Unit tests for judge module
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/59-llm-batch-caching/59-CONTEXT.md
@.planning/phases/59-llm-batch-caching/59-RESEARCH.md
@.planning/phases/59-llm-batch-caching/59-01-SUMMARY.md
@api/functions/llm-service.R
@api/functions/llm-cache-repository.R
@api/functions/llm-validation.R
@api/functions/llm-batch-generator.R
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create LLM-as-judge module</name>
  <files>api/functions/llm-judge.R</files>
  <action>
Create new file `api/functions/llm-judge.R` with:

1. **Module header and dependencies:**
   ```r
   # functions/llm-judge.R
   #
   # LLM-as-judge validation for cluster summaries.
   # Uses Gemini to evaluate summary accuracy and grounding.
   #
   # Key features:
   # - Three-tier verdict: accept, low_confidence, reject
   # - Same model as generation for consistency
   # - Stores verdict and reasoning in summary metadata

   require(ellmer)
   require(logger)
   require(glue)
   ```

2. **llm_judge_verdict_type:** Type specification for judge response
   ```r
   llm_judge_verdict_type <- ellmer::type_object(
     "Validation verdict for a cluster summary",

     is_factually_accurate = ellmer::type_boolean(
       "Summary accurately describes biological function of the genes"
     ),

     is_grounded = ellmer::type_boolean(
       "All claims are supported by the enrichment data provided"
     ),

     pathways_valid = ellmer::type_boolean(
       "Listed pathways match the enrichment input (no invented pathways)"
     ),

     confidence_appropriate = ellmer::type_boolean(
       "Self-assessed confidence matches the evidence strength"
     ),

     reasoning = ellmer::type_string(
       "Brief explanation of assessment (2-3 sentences)"
     ),

     verdict = ellmer::type_enum(
       c("accept", "low_confidence", "reject"),
       "Final verdict: accept (cache as validated), low_confidence (cache but flag),
        reject (do not cache, trigger regeneration)"
     )
   )
   ```

3. **validate_with_llm_judge(summary, cluster_data, model = "gemini-2.0-flash"):**
   - Build context for judge from cluster_data (genes, enrichment terms)
   - Construct evaluation prompt with:
     - Original cluster data (genes list, top 15 enrichment terms)
     - Generated summary to evaluate
     - Key themes claimed, pathways listed, self-assessed confidence
     - Clear evaluation criteria
   - Call ellmer chat$chat_structured() with llm_judge_verdict_type
   - Handle errors gracefully: if judge fails, return low_confidence verdict with fallback reasoning
   - Log verdict and reasoning
   - Return verdict result

4. **generate_and_validate_with_judge(cluster_data, cluster_type):**
   - Step 1: Call generate_cluster_summary() from llm-service.R (includes entity validation)
   - If generation fails, return early with error
   - Step 2: Call validate_with_llm_judge() on the summary
   - Step 3: Determine validation_status from verdict:
     - "accept" -> "validated"
     - "low_confidence" -> "pending" (usable but flagged)
     - "reject" -> "rejected"
   - Step 4: Add judge metadata to summary:
     - llm_judge_verdict
     - llm_judge_reasoning
     - derived_confidence (from calculate_derived_confidence)
   - Step 5: Save to cache with appropriate validation_status
   - Return result with success flag, summary, cache_id, validation_status, judge_result

5. **Key implementation details:**
   - Use same model for judging as for generation (consistency)
   - Keep judge prompt focused on grounding and accuracy
   - On "reject" verdict, function returns success = FALSE to trigger retry
   - Log all verdicts for calibration analysis
  </action>
  <verify>
Run: `grep -E "llm_judge_verdict_type|validate_with_llm_judge|generate_and_validate_with_judge" api/functions/llm-judge.R`
Expect: All three names present
Run: `grep "accept.*low_confidence.*reject" api/functions/llm-judge.R`
Expect: Three-tier verdict enum visible
  </verify>
  <done>
llm-judge.R created with llm_judge_verdict_type, validate_with_llm_judge(), and generate_and_validate_with_judge() implementing the full validation pipeline with three-tier verdicts.
  </done>
</task>

<task type="auto">
  <name>Task 2: Integrate judge into batch generator and add cache helper</name>
  <files>api/functions/llm-batch-generator.R, api/functions/llm-cache-repository.R</files>
  <action>
**Modify api/functions/llm-batch-generator.R:**

1. At the top of the file, add source for llm-judge.R:
   ```r
   # Load LLM judge module
   if (!exists("generate_and_validate_with_judge", mode = "function")) {
     if (file.exists("functions/llm-judge.R")) {
       source("functions/llm-judge.R", local = TRUE)
     }
   }
   ```

2. In llm_batch_executor(), replace the call to generate_cluster_summary() with generate_and_validate_with_judge():

   Find the section that calls generate_cluster_summary (or equivalent generation logic).
   Replace with:
   ```r
   result <- tryCatch({
     generate_and_validate_with_judge(cluster_data, cluster_type)
   }, error = function(e) {
     log_warn("Cluster {cluster_num} attempt {attempts}: {e$message}")
     list(success = FALSE, error = e$message)
   })
   ```

3. Update success tracking based on judge verdict:
   - If result$success = TRUE (accept or low_confidence): increment succeeded
   - If result$success = FALSE (reject or error): retry or increment failed

4. Log the validation_status with each cluster outcome:
   ```r
   log_info("Cluster {cluster_num}: {result$validation_status} (judge: {result$judge_result$verdict})")
   ```

**Modify api/functions/llm-cache-repository.R:**

5. Add update_validation_status() function (for manual admin updates if needed):
   ```r
   #' Update validation status of a cached summary
   #'
   #' Allows admins to manually update validation status.
   #'
   #' @param cache_id Integer, the cache_id to update
   #' @param validation_status Character, new status ("pending", "validated", "rejected")
   #' @param validated_by Character or NULL, admin user who validated
   #'
   #' @return Logical, TRUE if update succeeded
   #'
   #' @export
   update_validation_status <- function(cache_id, validation_status, validated_by = NULL) {
     valid_statuses <- c("pending", "validated", "rejected")
     if (!validation_status %in% valid_statuses) {
       log_error("Invalid validation_status: {validation_status}")
       return(FALSE)
     }

     validated_at <- if (validation_status == "validated") "NOW()" else "NULL"

     db_execute_statement(
       "UPDATE llm_cluster_summary_cache
        SET validation_status = ?,
            validated_at = CASE WHEN ? = 'validated' THEN NOW() ELSE NULL END,
            validated_by = ?
        WHERE cache_id = ?",
       list(validation_status, validation_status, validated_by, as.integer(cache_id))
     )

     log_info("Updated cache_id={cache_id} to validation_status={validation_status}")
     return(TRUE)
   }
   ```
  </action>
  <verify>
Run: `grep "generate_and_validate_with_judge" api/functions/llm-batch-generator.R`
Expect: Function call present in batch executor
Run: `grep "update_validation_status" api/functions/llm-cache-repository.R`
Expect: Function definition present
Run: `grep "llm-judge.R" api/functions/llm-batch-generator.R`
Expect: Source statement for judge module
  </verify>
  <done>
Batch generator now calls generate_and_validate_with_judge() instead of generate_cluster_summary(). LLM-as-judge is integrated into the pipeline. Cache repository has update_validation_status() for manual overrides.
  </done>
</task>

<task type="auto">
  <name>Task 3: Create unit tests for LLM-as-judge</name>
  <files>api/tests/testthat/test-llm-judge.R</files>
  <action>
Create new file `api/tests/testthat/test-llm-judge.R` with:

1. **Test setup:**
   ```r
   # Test file: test-llm-judge.R
   # Unit tests for LLM-as-judge validation functions

   skip_if_no_gemini <- function() {
     skip_if(!exists("is_gemini_configured", mode = "function") || !is_gemini_configured(),
             "GEMINI_API_KEY not configured")
   }

   skip_if_no_db <- function() {
     skip_if(!exists("db_execute_query", mode = "function"), "Database helpers not loaded")
   }
   ```

2. **Tests for llm_judge_verdict_type:**
   - Test: type object has required fields (verdict, reasoning, is_factually_accurate, etc.)
   - Test: verdict enum contains exactly accept, low_confidence, reject

3. **Tests for validate_with_llm_judge() logic:**
   - Test: handles NULL summary gracefully
   - Test: handles NULL cluster_data gracefully
   - Test: returns valid verdict structure with required fields
   - Test: verdict is one of accept/low_confidence/reject

4. **Tests for generate_and_validate_with_judge():**
   - Test: returns success=FALSE when generation fails
   - Test: returns correct validation_status mapping:
     - accept -> validated
     - low_confidence -> pending
     - reject -> rejected
   - Test: result contains judge_result with verdict and reasoning

5. **Tests for update_validation_status():**
   - Test: rejects invalid validation_status values
   - Test: returns FALSE for invalid status
   - Test: accepts valid status values (pending, validated, rejected)

6. **Integration test (requires API key and DB):**
   ```r
   test_that("full judge pipeline works end-to-end", {
     skip_if_no_gemini()
     skip_if_no_db()

     # Create minimal test cluster data
     cluster_data <- list(
       identifiers = tibble::tibble(
         hgnc_id = c(1234, 5678),
         symbol = c("BRCA1", "TP53")
       ),
       term_enrichment = tibble::tibble(
         category = "GO",
         term = c("DNA repair", "cell cycle"),
         fdr = c(1e-10, 1e-8)
       ),
       cluster_number = 1L
     )

     result <- generate_and_validate_with_judge(cluster_data, "functional")

     expect_true("success" %in% names(result))
     expect_true("validation_status" %in% names(result))
     expect_true("judge_result" %in% names(result))
   })
   ```

7. **Test patterns:**
   - Use skip_if_no_gemini() for tests requiring Gemini API
   - Use skip_if_no_db() for tests requiring database
   - Test error handling paths explicitly
   - Verify structure of returned objects
  </action>
  <verify>
Run: `wc -l api/tests/testthat/test-llm-judge.R`
Expect: At least 50 lines
Run: `grep -c "test_that" api/tests/testthat/test-llm-judge.R`
Expect: At least 6 test cases
Run: `grep "skip_if_no_gemini" api/tests/testthat/test-llm-judge.R`
Expect: Skip function defined and used
  </verify>
  <done>
Unit tests created in test-llm-judge.R covering verdict type, validate_with_llm_judge, generate_and_validate_with_judge, update_validation_status, and end-to-end integration. Tests properly skip when API/database not available.
  </done>
</task>

</tasks>

<verification>
1. llm-judge.R exists with all required exports
2. Batch generator uses generate_and_validate_with_judge()
3. Cache repository has update_validation_status()
4. Unit tests exist with at least 6 test cases
5. R files load without syntax errors:
   ```bash
   cd api && Rscript -e "source('functions/llm-judge.R')" 2>&1 | head -5
   ```
</verification>

<success_criteria>
1. validate_with_llm_judge() calls Gemini with judge prompt and returns structured verdict
2. generate_and_validate_with_judge() chains generation -> entity validation -> judge validation -> cache
3. Three-tier verdict (accept/low_confidence/reject) maps to validation_status (validated/pending/rejected)
4. Judge verdict and reasoning stored in summary metadata (llm_judge_verdict, llm_judge_reasoning)
5. Rejected summaries return success=FALSE to trigger retry in batch executor
6. Low_confidence summaries are cached but flagged (validation_status = pending)
7. Unit tests cover key scenarios including error handling
</success_criteria>

<output>
After completion, create `.planning/phases/59-llm-batch-caching/59-02-SUMMARY.md`
</output>
