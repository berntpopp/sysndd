---
phase: 71-viewlogs-database-filtering
plan: 04
type: execute
wave: 2
depends_on: ["71-02", "71-03"]
files_modified:
  - api/endpoints/logging_endpoints.R
autonomous: true

must_haves:
  truths:
    - "GET /api/logs endpoint uses get_logs_paginated() from repository"
    - "Response includes pagination metadata with totalCount and hasMore"
    - "Invalid filter columns return 400 with error_type: invalid_filter_error"
    - "Endpoint no longer calls collect() on logging table"
    - "Execution time is included in response metadata"
  artifacts:
    - path: "api/endpoints/logging_endpoints.R"
      provides: "Refactored logging endpoint"
      min_lines: 100
  key_links:
    - from: "api/endpoints/logging_endpoints.R"
      to: "api/functions/logging-repository.R"
      via: "source() and get_logs_paginated() call"
      pattern: "get_logs_paginated"
    - from: "api/endpoints/logging_endpoints.R"
      to: "api/functions/logging-repository.R"
      via: "validate_logging_column for error handling"
      pattern: "invalid_filter_error"
---

<objective>
Refactor the logging endpoint to use the new repository layer for database-side filtering.

Purpose: Replace the current implementation that loads all rows with collect() before filtering, with the new get_logs_paginated() function that filters in the database using WHERE/LIMIT/OFFSET. This fixes the memory issue (#152) where the endpoint loads 1M+ rows into R memory.

Output: Refactored `api/endpoints/logging_endpoints.R` that uses the repository pattern.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/71-viewlogs-database-filtering/71-RESEARCH.md

# Current implementation to refactor:
@api/endpoints/logging_endpoints.R

# New repository functions to use:
@api/functions/logging-repository.R

# Dependencies:
@.planning/phases/71-viewlogs-database-filtering/71-02-SUMMARY.md
@.planning/phases/71-viewlogs-database-filtering/71-03-SUMMARY.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add logging-repository.R source to endpoint file</name>
  <files>api/endpoints/logging_endpoints.R</files>
  <action>
Add the source statement for the logging repository at the top of logging_endpoints.R, after the existing imports/header.

Add near the top of the file (after the header comments, before the endpoint functions):

```r
# Load logging repository for database-side filtering
if (!exists("get_logs_paginated", mode = "function")) {
  if (file.exists("functions/logging-repository.R")) {
    source("functions/logging-repository.R", local = FALSE)
  }
}
```

This follows the same pattern used in llm-cache-repository.R for loading db-helpers.R.
  </action>
  <verify>
```bash
# Source statement exists
grep "logging-repository.R" api/endpoints/logging_endpoints.R
grep "get_logs_paginated" api/endpoints/logging_endpoints.R
```
  </verify>
  <done>
logging-repository.R is sourced at the top of logging_endpoints.R, making get_logs_paginated() available.
  </done>
</task>

<task type="auto">
  <name>Task 2: Refactor GET / endpoint to use repository</name>
  <files>api/endpoints/logging_endpoints.R</files>
  <action>
Refactor the `GET /` endpoint function to use `get_logs_paginated()` instead of the current collect() pattern.

**Current problematic code to replace:**
```r
# Get logs from database
logs_raw <- pool %>%
  tbl("logging") %>%
  collect()

# Sort & filter
logs_raw <- logs_raw %>%
  arrange(!!!rlang::parse_exprs(sort_exprs)) %>%
  filter(!!!rlang::parse_exprs(filter_exprs))
```

**New implementation:**

1. Change the endpoint parameters to support the new simpler filter format while maintaining backward compatibility:

```r
#* @param status:int Optional. Filter by HTTP status code.
#* @param request_method:str Optional. Filter by request method (GET, POST, etc.).
#* @param path_prefix:str Optional. Filter by path prefix.
#* @param timestamp_from:str Optional. Filter by minimum timestamp.
#* @param timestamp_to:str Optional. Filter by maximum timestamp.
#* @param page:int Page number (1-indexed). Default 1.
#* @param per_page:int Page size. Default 50.
#* @param sort_column:str Column to sort by. Default "id".
#* @param sort_direction:str Sort direction (ASC or DESC). Default "DESC".
```

2. Replace the endpoint implementation:

```r
#* @get /
function(req,
         res,
         status = NULL,
         request_method = NULL,
         path_prefix = NULL,
         timestamp_from = NULL,
         timestamp_to = NULL,
         page = 1,
         per_page = 50,
         sort_column = "id",
         sort_direction = "DESC",
         format = "json") {
  require_role(req, res, "Administrator")

  # Set serializer
  res$serializer <- serializers[[format]]

  # Start time for execution time tracking
  start_time <- Sys.time()

  # Use repository for database-side filtering (no collect())
  tryCatch(
    {
      result <- get_logs_paginated(
        status = status,
        request_method = request_method,
        path_prefix = path_prefix,
        timestamp_from = timestamp_from,
        timestamp_to = timestamp_to,
        page = as.integer(page),
        per_page = as.integer(per_page),
        sort_column = sort_column,
        sort_direction = sort_direction
      )

      # Compute execution time
      end_time <- Sys.time()
      execution_time <- as.character(paste0(round(end_time - start_time, 2), " secs"))

      # Add execution time to meta
      result$meta$executionTime <- execution_time
      result$meta$sort <- paste(sort_column, sort_direction)

      if (format == "xlsx") {
        creation_date <- strftime(
          as.POSIXlt(Sys.time(), "UTC", "%Y-%m-%dT%H:%M:%S"),
          "%Y-%m-%d_T%H-%M-%S"
        )
        base_filename <- str_replace_all(req$PATH_INFO, "\\/", "_") %>%
          str_replace_all("_api_", "")
        filename <- file.path(paste0(base_filename, "_", creation_date, ".xlsx"))

        bin <- generate_xlsx_bin(result, base_filename)
        as_attachment(bin, filename)
      } else {
        result
      }
    },
    invalid_filter_error = function(e) {
      # Return 400 for invalid filter/sort column (LOG-04)
      res$status <- 400
      list(
        error = "INVALID_FILTER",
        error_type = "invalid_filter_error",
        message = e$message
      )
    },
    error = function(e) {
      # Return 500 for other errors
      res$status <- 500
      list(
        error = "INTERNAL_ERROR",
        message = paste("Failed to fetch logs:", e$message)
      )
    }
  )
}
```

Key changes:
- Removes `pool %>% tbl("logging") %>% collect()` pattern (the root cause of #152)
- Uses `get_logs_paginated()` for database-side filtering
- Returns 400 with invalid_filter_error for bad columns (LOG-04)
- Simpler parameters instead of complex filter string
- Maintains xlsx export support
  </action>
  <verify>
```bash
# No collect() in endpoint (the main fix)
! grep "collect()" api/endpoints/logging_endpoints.R && echo "No collect() - good!"

# Uses get_logs_paginated
grep "get_logs_paginated" api/endpoints/logging_endpoints.R

# Has tryCatch with invalid_filter_error handler
grep "invalid_filter_error" api/endpoints/logging_endpoints.R

# Returns 400 for invalid filter
grep "res\$status <- 400" api/endpoints/logging_endpoints.R
```
  </verify>
  <done>
GET / endpoint refactored to:
- Use get_logs_paginated() instead of collect() (LOG-01)
- Return 400 with invalid_filter_error for bad columns (LOG-04)
- Include pagination metadata in response
- Maintain xlsx export compatibility
  </done>
</task>

<task type="auto">
  <name>Task 3: Update endpoint documentation</name>
  <files>api/endpoints/logging_endpoints.R</files>
  <action>
Update the roxygen-style documentation for the GET endpoint to reflect the new parameters and response format:

```r
#* Get Paginated Log Entries
#*
#* This endpoint returns paginated log entries with database-side filtering.
#*
#* # `Details`
#* Queries the logging table with optional filters. All filtering is performed
#* in the database using WHERE clauses and LIMIT/OFFSET pagination, ensuring
#* efficient handling of large log tables.
#*
#* # `Return`
#* An offset pagination object with `data` and `meta`.
#*
#* @tag logging
#* @serializer json list(na="string")
#*
#* @param status:int Optional. Filter by HTTP status code (exact match).
#* @param request_method:str Optional. Filter by request method (GET, POST, etc.).
#* @param path_prefix:str Optional. Filter by path prefix (uses LIKE 'prefix%').
#* @param timestamp_from:str Optional. Filter logs >= this timestamp.
#* @param timestamp_to:str Optional. Filter logs <= this timestamp.
#* @param page:int Page number (1-indexed). Default 1.
#* @param per_page:int Rows per page. Default 50.
#* @param sort_column:str Column to sort by. Default "id". Allowed: id, timestamp, status, duration, path, request_method.
#* @param sort_direction:str Sort direction (ASC or DESC). Default "DESC".
#* @param format:str Output format ("json" or "xlsx").
#*
#* @response 200 OK. Pagination object with log entries and metadata (totalCount, pageSize, offset, currentPage, totalPages, hasMore).
#* @response 400 Bad Request. Invalid filter or sort column.
#* @response 403 Forbidden. If user not Admin.
#* @response 500 Internal server error.
```
  </action>
  <verify>
```bash
# Updated documentation mentions new params
grep "database-side filtering" api/endpoints/logging_endpoints.R
grep "totalCount" api/endpoints/logging_endpoints.R
grep "hasMore" api/endpoints/logging_endpoints.R
grep "@response 400" api/endpoints/logging_endpoints.R
```
  </verify>
  <done>
Endpoint documentation updated to reflect:
- New filter parameters (status, request_method, path_prefix, timestamp_from/to)
- Pagination parameters (page, per_page, sort_column, sort_direction)
- Response format with pagination metadata
- 400 response for invalid filters
  </done>
</task>

</tasks>

<verification>
1. Endpoint no longer uses `pool %>% tbl("logging") %>% collect()` pattern
2. Endpoint calls `get_logs_paginated()` from repository
3. Invalid columns return 400 with error_type: invalid_filter_error (LOG-04)
4. Response includes pagination metadata (totalCount, hasMore, etc.)
5. xlsx export still works
6. lintr passes on modified file
</verification>

<success_criteria>
- GET /api/logs uses database-side filtering (LOG-01)
- Invalid filter columns return 400 with invalid_filter_error (LOG-04)
- Response includes pagination metadata (PAG-02)
- No collect() before filter in endpoint
- All existing functionality (xlsx export) preserved
- `make lint-api` passes
</success_criteria>

<output>
After completion, create `.planning/phases/71-viewlogs-database-filtering/71-04-SUMMARY.md`
</output>
