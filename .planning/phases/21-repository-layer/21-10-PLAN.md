---
phase: 21-repository-layer
plan: 10
type: execute
wave: 1
depends_on: []
files_modified:
  - api/endpoints/admin_endpoints.R
  - api/functions/pubtator-functions.R
autonomous: true
gap_closure: true

must_haves:
  truths:
    - "Admin endpoint transactions use db_execute_statement inside db_with_transaction"
    - "Pubtator transactions use db_with_transaction with db_execute_query/db_execute_statement"
    - "Zero poolCheckout/dbExecute/dbGetQuery/dbAppendTable/dbBegin/dbCommit/dbRollback calls remain"
  artifacts:
    - path: "api/endpoints/admin_endpoints.R"
      provides: "Admin transactions using db-helpers"
      contains: "db_execute_statement"
    - path: "api/functions/pubtator-functions.R"
      provides: "Pubtator transactions using db-helpers"
      contains: "db_with_transaction"
  key_links:
    - from: "admin_endpoints.R"
      to: "db-helpers.R"
      via: "db_with_transaction + db_execute_statement"
      pattern: "db_with_transaction\\("
    - from: "pubtator-functions.R"
      to: "db-helpers.R"
      via: "db_with_transaction + db_execute_query"
      pattern: "db_with_transaction\\("
---

<objective>
Refactor admin_endpoints.R and pubtator-functions.R to use db-helpers for all database operations, eliminating direct DBI calls (poolCheckout, dbExecute, dbGetQuery, dbAppendTable, dbBegin, dbCommit, dbRollback).

Purpose: Close verification gap - these files have complex transactions that bypass db-helpers
Output: Both files use db_with_transaction with db_execute_query/db_execute_statement consistently
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/21-repository-layer/21-VERIFICATION.md
@api/functions/db-helpers.R
</context>

<tasks>

<task type="auto">
  <name>Task 1: Refactor admin_endpoints.R ontology and HGNC update transactions</name>
  <files>api/endpoints/admin_endpoints.R</files>
  <action>
The file has two functions with direct DBI calls INSIDE db_with_transaction blocks. The current code manually checks out a connection inside db_with_transaction, which defeats the purpose.

**CRITICAL ARCHITECTURE UNDERSTANDING:**

Looking at db-helpers.R, db_with_transaction already:
1. Checks out a connection from the global pool (line 233)
2. Registers poolReturn via on.exit (line 236)
3. Wraps code in DBI::dbWithTransaction for automatic commit/rollback

The db_execute_statement and db_execute_query functions use the GLOBAL `pool` directly (not a checked-out connection). This means:
- Inside db_with_transaction, call db_execute_statement directly - it uses the pool
- Do NOT call poolCheckout inside db_with_transaction
- Do NOT call poolReturn inside db_with_transaction
- The db_with_transaction handles its own connection for the transaction context

**Function 1: update_ontology (lines 140-166)**

Current broken pattern:
```r
db_with_transaction({
  conn <- poolCheckout(pool)           # WRONG - already inside transaction
  on.exit(poolReturn(conn), add = TRUE) # WRONG - db_with_transaction handles this
  dbExecute(conn, "SET FOREIGN_KEY_CHECKS = 0;")
  dbExecute(conn, "TRUNCATE TABLE disease_ontology_set;")
  dbAppendTable(conn, "disease_ontology_set", disease_ontology_set_update)
  dbExecute(conn, "TRUNCATE TABLE ndd_entity;")
  dbAppendTable(conn, "ndd_entity", ndd_entity_mutated)
  dbExecute(conn, "SET FOREIGN_KEY_CHECKS = 1;")
})
```

Replace with db_execute_statement calls (NO manual poolCheckout/poolReturn):
```r
db_with_transaction({
  db_execute_statement("SET FOREIGN_KEY_CHECKS = 0")
  db_execute_statement("TRUNCATE TABLE disease_ontology_set")

  # Insert disease_ontology_set rows using dynamic column names
  if (nrow(disease_ontology_set_update) > 0) {
    cols <- names(disease_ontology_set_update)
    placeholders <- paste(rep("?", length(cols)), collapse = ", ")
    sql <- sprintf("INSERT INTO disease_ontology_set (%s) VALUES (%s)",
                   paste(cols, collapse = ", "), placeholders)
    for (i in seq_len(nrow(disease_ontology_set_update))) {
      db_execute_statement(sql, as.list(disease_ontology_set_update[i, ]))
    }
  }

  db_execute_statement("TRUNCATE TABLE ndd_entity")

  # Insert ndd_entity rows using dynamic column names
  if (nrow(ndd_entity_mutated) > 0) {
    cols <- names(ndd_entity_mutated)
    placeholders <- paste(rep("?", length(cols)), collapse = ", ")
    sql <- sprintf("INSERT INTO ndd_entity (%s) VALUES (%s)",
                   paste(cols, collapse = ", "), placeholders)
    for (i in seq_len(nrow(ndd_entity_mutated))) {
      db_execute_statement(sql, as.list(ndd_entity_mutated[i, ]))
    }
  }

  db_execute_statement("SET FOREIGN_KEY_CHECKS = 1")
})
```

**Function 2: update_hgnc_data (lines 200-222)**

Current broken pattern:
```r
db_with_transaction({
  conn <- poolCheckout(pool)
  on.exit(poolReturn(conn), add = TRUE)
  dbExecute(conn, "SET FOREIGN_KEY_CHECKS = 0;")
  dbExecute(conn, "TRUNCATE TABLE non_alt_loci_set;")
  dbWriteTable(conn, "non_alt_loci_set", hgnc_data, append = TRUE)
  dbExecute(conn, "SET FOREIGN_KEY_CHECKS = 1;")
})
```

Replace with:
```r
db_with_transaction({
  db_execute_statement("SET FOREIGN_KEY_CHECKS = 0")
  db_execute_statement("TRUNCATE TABLE non_alt_loci_set")

  # Insert hgnc_data rows using dynamic column names
  if (nrow(hgnc_data) > 0) {
    cols <- names(hgnc_data)
    placeholders <- paste(rep("?", length(cols)), collapse = ", ")
    sql <- sprintf("INSERT INTO non_alt_loci_set (%s) VALUES (%s)",
                   paste(cols, collapse = ", "), placeholders)
    for (i in seq_len(nrow(hgnc_data))) {
      db_execute_statement(sql, as.list(hgnc_data[i, ]))
    }
  }

  db_execute_statement("SET FOREIGN_KEY_CHECKS = 1")
})
```

Add source at top if not present:
```r
source("functions/db-helpers.R", local = TRUE)
```

**Key understanding:** db_with_transaction isolates the transaction. db_execute_statement uses the global pool for its queries. This is the intended pattern - the transaction wrapper ensures atomicity while db_execute_statement provides the parameterized query interface.
  </action>
  <verify>
1. `grep -n "poolCheckout\|poolReturn\|dbExecute\|dbAppendTable\|dbWriteTable" api/endpoints/admin_endpoints.R` returns empty
2. `grep -n "db_execute_statement\|db_with_transaction" api/endpoints/admin_endpoints.R` returns multiple matches
3. Syntax check: `R -e "source('api/endpoints/admin_endpoints.R', local=TRUE)"` - no errors
4. Verify NO poolCheckout inside db_with_transaction blocks: `grep -A5 "db_with_transaction" api/endpoints/admin_endpoints.R | grep -v poolCheckout`
  </verify>
  <done>Admin endpoint transactions use db_execute_statement inside db_with_transaction, no manual poolCheckout/poolReturn, no direct DBI calls</done>
</task>

<task type="auto">
  <name>Task 2: Migrate pubtator-functions.R SELECT queries to db_execute_query</name>
  <files>api/functions/pubtator-functions.R</files>
  <action>
This task focuses on migrating SELECT queries (dbGetQuery) to db_execute_query. The transaction management migration is handled in Task 3.

**Replace all dbGetQuery calls with db_execute_query:**

**Line 131-137 - Check existing query:**
```r
# Before:
existing_query <- dbGetQuery(
  db_conn,
  "SELECT query_id, queried_page_number, total_page_number, page_size
   FROM pubtator_query_cache
   WHERE query_hash = ?",
  params = list(q_hash)
)

# After:
existing_query <- db_execute_query(
  "SELECT query_id, queried_page_number, total_page_number, page_size
   FROM pubtator_query_cache
   WHERE query_hash = ?",
  list(q_hash)
)
```

**Line 151 - Get LAST_INSERT_ID:**
```r
# Before:
query_id <- dbGetQuery(db_conn, "SELECT LAST_INSERT_ID() AS id;")$id[1]

# After:
query_id <- db_execute_query("SELECT LAST_INSERT_ID() AS id")$id[1]
```

**Line 248-253 - Get PMIDs:**
```r
# Before:
pmid_rows <- dbGetQuery(db_conn, "
  SELECT pmid
  FROM pubtator_search_cache
  WHERE query_id=? AND pmid IS NOT NULL
  GROUP BY pmid
", params = list(query_id))

# After:
pmid_rows <- db_execute_query(
  "SELECT pmid FROM pubtator_search_cache WHERE query_id=? AND pmid IS NOT NULL GROUP BY pmid",
  list(query_id)
)
```

**Line 274-278 - Get search mapping:**
```r
# Before:
srch_map <- dbGetQuery(db_conn, "
  SELECT search_id, pmid
  FROM pubtator_search_cache
  WHERE query_id=?
", params = list(query_id))

# After:
srch_map <- db_execute_query(
  "SELECT search_id, pmid FROM pubtator_search_cache WHERE query_id=?",
  list(query_id)
)
```

Add source at top of file:
```r
source("functions/db-helpers.R", local = TRUE)
```

**Note:** After this task, the file will have a mix of db_execute_query (new) and direct DBI calls for INSERT/UPDATE/DELETE and transaction management. Task 3 will complete the migration.
  </action>
  <verify>
1. `grep -n "dbGetQuery" api/functions/pubtator-functions.R` returns empty
2. `grep -n "db_execute_query" api/functions/pubtator-functions.R` returns 4+ matches
3. Syntax check: `R -e "source('api/functions/pubtator-functions.R', local=TRUE)"` - no errors
  </verify>
  <done>All SELECT queries in pubtator-functions.R use db_execute_query instead of dbGetQuery</done>
</task>

<task type="auto">
  <name>Task 3: Migrate pubtator-functions.R transaction management and INSERT/UPDATE/DELETE</name>
  <files>api/functions/pubtator-functions.R</files>
  <action>
This task restructures the pubtator_db_update function to use db_with_transaction and db_execute_statement, handling the complex early return logic.

**CRITICAL ARCHITECTURE CONSIDERATIONS:**

1. **Early returns (lines 117, 189, 257, 267):** The current code has return statements inside the transaction that call dbCommit or dbRollback first. With db_with_transaction:
   - Success: Just return the value - db_with_transaction auto-commits
   - Failure: Throw an error or use stop() - db_with_transaction auto-rolls back

2. **on.exit poolReturn cleanup (lines 97-104):** REMOVE this. db_with_transaction handles its own connection.

3. **dbBegin/dbCommit/dbRollback:** REMOVE ALL. db_with_transaction handles this via DBI::dbWithTransaction.

**RESTRUCTURED FUNCTION:**

```r
pubtator_db_update <- function(
    db_host, db_port, db_name, db_user, db_password,
    query, max_pages = 10, do_full_update = FALSE
) {
  # A) Retrieve total_pages BEFORE transaction (no DB write needed)
  total_pages <- pubtator_v3_total_pages_from_query(query)
  if (is.null(total_pages) || total_pages == 0) {
    log_warn("No pages found for query: {query}. Aborting.")
    return(NULL)  # Early return - no DB operations started
  }

  if (max_pages > total_pages) {
    log_info("max_pages={max_pages} > total_pages={total_pages}, adjusting.")
    max_pages <- total_pages
  }

  # B) Query hash
  q_hash <- generate_query_hash(query)
  log_info("Query hash = {q_hash}")

  # C) Wrap ALL database operations in a single transaction
  result <- tryCatch({
    db_with_transaction({
      # Check if query exists
      existing_query <- db_execute_query(
        "SELECT query_id, queried_page_number, total_page_number, page_size
         FROM pubtator_query_cache WHERE query_hash = ?",
        list(q_hash)
      )

      query_id <- NA_integer_
      old_queried_number <- 0

      if (nrow(existing_query) == 0) {
        # Insert new row
        log_info("No record for query_hash={q_hash}, inserting new row.")
        db_execute_statement(
          "INSERT INTO pubtator_query_cache
            (query_text, query_hash, total_page_number, queried_page_number, page_size)
           VALUES (?, ?, ?, ?, ?)",
          list(query, q_hash, total_pages, max_pages, 10)
        )
        query_id <- db_execute_query("SELECT LAST_INSERT_ID() AS id")$id[1]

      } else {
        # Found existing row
        query_id <- existing_query$query_id[1]
        old_queried_number <- existing_query$queried_page_number[1]
        old_total_number <- existing_query$total_page_number[1]

        log_info("Found existing query_id={query_id}, old_queried_number={old_queried_number}")

        if (do_full_update) {
          log_info("do_full_update=TRUE => removing old records.")
          db_execute_statement(
            "DELETE FROM pubtator_search_cache WHERE query_id = ?",
            list(query_id)
          )
          db_execute_statement(
            "DELETE a FROM pubtator_annotation_cache a
             JOIN pubtator_search_cache s ON a.search_id = s.search_id
             WHERE s.query_id = ?",
            list(query_id)
          )
          db_execute_statement(
            "UPDATE pubtator_query_cache
             SET total_page_number=?, queried_page_number=?, page_size=?
             WHERE query_id=?",
            list(total_pages, max_pages, 10, query_id)
          )
          old_queried_number <- 0

        } else {
          # Partial update check
          if (max_pages <= old_queried_number) {
            log_info("Already up to page={old_queried_number}, no new fetch needed.")
            # Return query_id - transaction will auto-commit
            return(query_id)  # Early return WITH commit
          }
          log_info("Partial update: old_queried_number={old_queried_number}, new max_pages={max_pages}")
        }
      }

      # D) Fetch new pages
      start_page <- if (!do_full_update && nrow(existing_query) > 0) {
        existing_query$queried_page_number[1] + 1
      } else {
        1
      }

      if (start_page <= max_pages) {
        df_results <- pubtator_v3_pmids_from_request(
          query = query, start_page = start_page,
          max_pages = (max_pages - start_page + 1)
        )

        if (!is.null(df_results) && nrow(df_results) > 0) {
          log_info("Inserting {nrow(df_results)} rows => pubtator_search_cache")

          df_insert <- df_results %>%
            mutate(
              query_id = query_id,
              id = if (!"id" %in% names(.)) NA_character_ else id,
              date = if ("date" %in% names(.)) sub("T.*", "", date) else NA_character_
            ) %>%
            select(query_id, id, pmid, doi, title, journal, date, score, text_hl)

          for (r in seq_len(nrow(df_insert))) {
            db_execute_statement(
              "INSERT INTO pubtator_search_cache
                (query_id, id, pmid, doi, title, journal, date, score, text_hl)
               VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)",
              unname(as.list(df_insert[r, ]))
            )
          }

          db_execute_statement(
            "UPDATE pubtator_query_cache
             SET queried_page_number=?, total_page_number=? WHERE query_id=?",
            list(max_pages, total_pages, query_id)
          )
        }
      }

      # E) Gather PMIDs and fetch annotations
      pmid_rows <- db_execute_query(
        "SELECT pmid FROM pubtator_search_cache
         WHERE query_id=? AND pmid IS NOT NULL GROUP BY pmid",
        list(query_id)
      )

      if (nrow(pmid_rows) == 0) {
        log_info("No PMIDs in search_cache => skip annotation fetch.")
        return(query_id)  # Early return WITH commit
      }

      pmid_vector <- pmid_rows$pmid
      log_info("Found {length(pmid_vector)} PMIDs => fetching annotations...")

      # F) Fetch and insert annotations
      doc_list <- pubtator_v3_data_from_pmids(pmid_vector)
      if (is.null(doc_list) || length(doc_list) == 0) {
        log_warn("No annotation data => skipping annotation_cache insert.")
        return(query_id)  # Early return WITH commit
      }

      flat_df <- flatten_pubtator_passages(doc_list) %>%
        mutate(pmid = as.integer(pmid))

      srch_map <- db_execute_query(
        "SELECT search_id, pmid FROM pubtator_search_cache WHERE query_id=?",
        list(query_id)
      )

      flat_df_j <- flat_df %>%
        left_join(srch_map, by = "pmid", relationship = "many-to-many")

      log_info("Inserting {nrow(flat_df_j)} annotation rows")

      df_ann <- flat_df_j %>%
        mutate(valid = if_else(valid == "TRUE", 1, 0, missing = 0)) %>%
        select(search_id, pmid, id, text, identifier, type, ncbi_homologene,
               valid, normalized, `database`, normalized_id, biotype, name, accession)

      for (r in seq_len(nrow(df_ann))) {
        db_execute_statement(
          "INSERT INTO pubtator_annotation_cache
            (search_id, pmid, id, text, identifier, type, ncbi_homologene, valid,
             normalized, `database`, normalized_id, biotype, name, accession)
           VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)",
          unname(as.list(df_ann[r, ]))
        )
      }

      log_info("All done => returning query_id={query_id}")
      query_id  # Return value - transaction auto-commits
    })
  }, error = function(e) {
    # Transaction auto-rolled back by db_with_transaction
    log_error("pubtator_db_update: Error => {e$message}")
    return(NULL)
  })

  return(result)
}
```

**KEY CHANGES:**

1. **Moved total_pages check BEFORE transaction** - no DB ops needed, so check first
2. **Single db_with_transaction wraps ALL DB operations** - atomic
3. **Early returns inside db_with_transaction just return the value** - auto-commits
4. **REMOVED: poolCheckout, poolReturn, on.exit cleanup, dbBegin, dbCommit, dbRollback**
5. **All dbGetQuery -> db_execute_query** (done in Task 2)
6. **All dbExecute/dbSendQuery+dbBind -> db_execute_statement**
7. **tryCatch around db_with_transaction** - errors auto-rollback, we just log and return NULL
  </action>
  <verify>
1. `grep -n "poolCheckout\|poolReturn\|dbBegin\|dbCommit\|dbRollback\|dbExecute\|dbSendQuery\|dbBind\|dbClearResult" api/functions/pubtator-functions.R` returns empty
2. `grep -n "db_with_transaction" api/functions/pubtator-functions.R` returns 1 match (the outer wrapper)
3. `grep -n "db_execute_statement" api/functions/pubtator-functions.R` returns 7+ matches
4. Syntax check: `R -e "source('api/functions/pubtator-functions.R', local=TRUE)"` - no errors
5. Verify early returns are inside db_with_transaction (they will auto-commit)
  </verify>
  <done>Pubtator functions use db_with_transaction with db_execute_query/db_execute_statement, no direct DBI calls, early returns handled correctly with auto-commit</done>
</task>

</tasks>

<verification>
Run comprehensive gap verification:
```bash
# Check ALL four files have no direct DBI calls (include 21-09 files too)
echo "=== Checking for remaining direct DBI calls ==="
grep -rn "dbConnect\|poolCheckout\|poolReturn\|dbBegin\|dbCommit\|dbRollback\|dbGetQuery\|dbExecute\|dbSendQuery\|dbBind\|dbClearResult\|dbAppendTable\|dbWriteTable\|poolWithTransaction" \
  api/endpoints/authentication_endpoints.R \
  api/endpoints/admin_endpoints.R \
  api/functions/pubtator-functions.R \
  api/functions/publication-functions.R

# Should return empty (or only comments/documentation)

echo "=== Checking db-helpers usage ==="
grep -rn "db_execute_query\|db_execute_statement\|db_with_transaction" \
  api/endpoints/authentication_endpoints.R \
  api/endpoints/admin_endpoints.R \
  api/functions/pubtator-functions.R \
  api/functions/publication-functions.R

# Should return multiple matches in each file

echo "=== Verifying no poolCheckout inside db_with_transaction ==="
grep -A10 "db_with_transaction" api/endpoints/admin_endpoints.R api/functions/pubtator-functions.R | grep poolCheckout
# Should return empty
```
</verification>

<success_criteria>
- Zero direct DBI calls in admin_endpoints.R (poolCheckout, dbExecute, dbAppendTable, dbWriteTable)
- Zero direct DBI calls in pubtator-functions.R (poolCheckout, dbBegin, dbCommit, dbRollback, dbGetQuery, dbExecute, dbSendQuery, dbBind, dbClearResult)
- Both files use db_with_transaction for transactional operations
- Both files use db_execute_query for SELECT operations
- Both files use db_execute_statement for INSERT/UPDATE/DELETE operations
- Both files source db-helpers.R
- No manual poolCheckout/poolReturn inside db_with_transaction blocks
- Early returns in pubtator_db_update handled correctly (auto-commit within transaction)
- Existing functionality preserved (same data operations)
</success_criteria>

<output>
After completion, create `.planning/phases/21-repository-layer/21-10-SUMMARY.md`
</output>
